{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Tuberculosis Detection using Chest X-ray with CNNs, Lung Segmentation + LIME (0.99 accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["# Outlines\n","- 1. Data Preprocessing\n","    - 1.1 Data Transforming\n","    - 1.2 Target\n","    - 1.3 Data Augmentation (Class imbalance problem) (Optional)\n","    - 1.4 Data Preparation\n","    - 1.5 Image Enhancement\n","        - 1.5.1 Contrast Stretching\n","        - 1.5.2 Histogram Equalization\n","        - 1.5.3 Adaptive Histogram Equalization\n","        - 1.5.4 Summary of each image enhancement\n","\n","2. CNN:\n","    - 2.1 How CNNs Work\n","    - 2.2 Layers\n","        - 2.2.1 Convolutional Layers\n","        - 2.2.2 Pooling Layers\n","        - 2.2.3 Dropout Layer\n","        - 2.2.4 Flatten Layer\n","        - 2.2.5 Fully Connected (Dense) Layers\n","        - 2.2.6 Output Layers\n","    - 2.3 Hyperparameters in both convolutional and pooling layers\n","        - 2.3.1 Padding\n","        - 2.3.2 Kernel size\n","        - 2.3.3 Stride\n","    - 2.4 Activation Functions\n","        - 2.4.1 ReLU Activation function\n","        - 2.4.2 Sigmoid Activation function\n","\n","3. Splitting the Dataset\n","    80:10:10\n","4. Modeling\n","\n","5. Evaluation\n","    - 5.1 Training set Performance\n","    - 5.2 Test set Performance\n","    - 5.3 Confusion matrix\n","    - 5.4 ROC\n","\n","6. Different Image Input Sizes\n","\n","7. Model Prediction\n","\n","8. Lung segmentation\n","\n","9. Explainability\n","    - 9.1 Lime Library"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Statment\n","Tuberculosis (TB) remains a significant global health issue, affecting millions of people worldwide. Timely and accurate diagnosis is crucial for effective management and control of the disease. Chest X-ray (CXR) imaging is a common diagnostic tool for TB, providing detailed insights into the condition of the lungs. Convolutional Neural Networks (CNNs) have demonstrated remarkable capabilities in image classification tasks, making them a promising technology for automated TB detection."]},{"cell_type":"markdown","metadata":{},"source":["# Problem in dataset\n","The dataset consists of:\n","- Amount: 700 Tuberculosis images /  3500 normal images. (Class imbalance)\n","- File type .PNG\n","- Image size: 512x512 \n","- Channel: 3 channel (RGB )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:47.64116Z","iopub.status.busy":"2023-11-24T08:41:47.640727Z","iopub.status.idle":"2023-11-24T08:41:47.67043Z","shell.execute_reply":"2023-11-24T08:41:47.669555Z","shell.execute_reply.started":"2023-11-24T08:41:47.641092Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### View the variables \n","<u>Read</u> 5 Training examples, And <u>check</u> the dimensions of raw data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:47.673432Z","iopub.status.busy":"2023-11-24T08:41:47.672689Z","iopub.status.idle":"2023-11-24T08:41:49.013032Z","shell.execute_reply":"2023-11-24T08:41:49.011965Z","shell.execute_reply.started":"2023-11-24T08:41:47.673391Z"},"trusted":true},"outputs":[],"source":["# range(start, stop, step)\n","for i in range(1,5):\n","    image = Image.open(\n","        f'../input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/Normal/Normal-{i}.png')\n","    print(image.mode)\n","    print(image.size)\n","    plt.imshow(image)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":[" ## 1.1 Data Transforming\n"," Objective: (Tabulating) Converting **Image.PNG** Files to **NumPy Arrays**\n"," - **Pros:**\n","    - **Memory Efficiency:** NumPy arrays require less memory than raw image files, making it ideal for large datasets.\n","    - **Computational Efficiency:** NumPy array operations are optimized and faster.\n","    \n","- **Cons:**\n","    - **Lossy Compression:** Depending on the image format and compression settings used, converting images to NumPy arrays may involve lossy compression, which can result in some loss of image quality.\n","    - **Storage and Disk Space**: While NumPy arrays can be more memory-efficient, they can still consume a significant amount of disk space, especially for high-resolution images or a large number of images."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:49.015321Z","iopub.status.busy":"2023-11-24T08:41:49.014622Z","iopub.status.idle":"2023-11-24T08:41:49.026537Z","shell.execute_reply":"2023-11-24T08:41:49.025737Z","shell.execute_reply.started":"2023-11-24T08:41:49.015279Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from numpy import asarray\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:49.029694Z","iopub.status.busy":"2023-11-24T08:41:49.029019Z","iopub.status.idle":"2023-11-24T08:41:49.034472Z","shell.execute_reply":"2023-11-24T08:41:49.033444Z","shell.execute_reply.started":"2023-11-24T08:41:49.029652Z"},"trusted":true},"outputs":[],"source":["dir_path = '../input/tuberculosis-tb-chest-xray-dataset/TB_Chest_Radiography_Database/'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:49.038371Z","iopub.status.busy":"2023-11-24T08:41:49.037622Z","iopub.status.idle":"2023-11-24T08:41:49.047198Z","shell.execute_reply":"2023-11-24T08:41:49.04639Z","shell.execute_reply.started":"2023-11-24T08:41:49.038331Z"},"trusted":true},"outputs":[],"source":["def numpize(subset: str, img_size=(128, 128), grayscale=False):\n","    \"\"\"\n","    Convert a set of images from a specified subset to NumPy arrays.\n","\n","    Parameters:\n","    - subset (str): Specifies the subset of images, e.g., 'Normal' or 'Tuberculosis'.\n","    - img_size (tuple): Tuple representing the desired size of the images after resizing.\n","    - grayscale (bool): If True, convert images to grayscale.\n","\n","    Returns:\n","    - numpy_array (numpy.ndarray): NumPy array containing the transformed images.\n","    \"\"\"\n","    image_dir = f'{dir_path}/{subset}/'\n","    image_files = os.listdir(image_dir)\n","    image_count = len(image_files)\n","    transformed_images = []\n","\n","    for i in tqdm(range(1, image_count + 1), desc='Transforming...'):\n","        image_path = f'{image_dir}/{subset}-{i}.png'\n","        image = Image.open(image_path)\n","\n","        # Ensure all images are in the same format\n","        if grayscale:\n","            image = image.convert('L')  # Convert to grayscale\n","        else:\n","            image = image.convert('RGB')  # Convert to RGB\n","\n","        # Resize all images to the specified size without padding\n","        resized_image = image.resize(img_size)\n","        data = np.asarray(resized_image)\n","        transformed_images.append(data)\n","\n","    numpy_array = np.array(transformed_images)\n","    return numpy_array"]},{"cell_type":"markdown","metadata":{},"source":["## Normal"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:41:49.049695Z","iopub.status.busy":"2023-11-24T08:41:49.048886Z","iopub.status.idle":"2023-11-24T08:42:53.647186Z","shell.execute_reply":"2023-11-24T08:42:53.646022Z","shell.execute_reply.started":"2023-11-24T08:41:49.049654Z"},"trusted":true},"outputs":[],"source":["norm = numpize(subset='Normal', \n","               img_size=(128, 128), \n","               grayscale=False)\n","norm.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Tuberculosis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:42:53.649128Z","iopub.status.busy":"2023-11-24T08:42:53.648764Z","iopub.status.idle":"2023-11-24T08:43:05.193705Z","shell.execute_reply":"2023-11-24T08:43:05.192637Z","shell.execute_reply.started":"2023-11-24T08:42:53.649098Z"},"trusted":true},"outputs":[],"source":["tb = numpize(subset='Tuberculosis', \n","             img_size=(128, 128), \n","             grayscale=False)\n","tb.shape"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Target"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:05.196269Z","iopub.status.busy":"2023-11-24T08:43:05.195057Z","iopub.status.idle":"2023-11-24T08:43:05.203498Z","shell.execute_reply":"2023-11-24T08:43:05.201651Z","shell.execute_reply.started":"2023-11-24T08:43:05.196227Z"},"trusted":true},"outputs":[],"source":["target_norm = np.zeros(norm.shape[0])\n","print(f'Shape: {target_norm.shape[0]}, array: {target_norm}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:05.208085Z","iopub.status.busy":"2023-11-24T08:43:05.20734Z","iopub.status.idle":"2023-11-24T08:43:05.227325Z","shell.execute_reply":"2023-11-24T08:43:05.22603Z","shell.execute_reply.started":"2023-11-24T08:43:05.208049Z"},"trusted":true},"outputs":[],"source":["target_tb = np.ones(tb.shape[0])\n","print(f'Shape: {target_tb.shape[0]}, array: {target_tb}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:05.229429Z","iopub.status.busy":"2023-11-24T08:43:05.228808Z","iopub.status.idle":"2023-11-24T08:43:05.242969Z","shell.execute_reply":"2023-11-24T08:43:05.241869Z","shell.execute_reply.started":"2023-11-24T08:43:05.229393Z"},"trusted":true},"outputs":[],"source":["def show_image(images, target, title, num_display=16, num_cols=4, cmap='gray', random_mode=False):\n","    '''\n","    :Parameters\n","        images (ndarray (n,)): Input data as a numpy array.\n","        target (ndarray (n,)): Target data as a numpy array.\n","        title (String): Title of the plot.\n","        num_display (int): Number of images to display. Default is 16.\n","        num_cols (int): Number of columns in the plot. Default is 4.\n","        cmap (str): Color map for displaying images. Default is 'gray'.\n","        random_mode (bool): If True, display images randomly. If False, display the first num_display images. Default is False.\n","    '''\n","    # Determine the number of rows based on the num_cols parameter\n","    n_cols = min(num_cols, num_display)\n","    n_rows = int(np.ceil(num_display / n_cols))\n","\n","    n_images = min(num_display, len(images))\n","    if random_mode:\n","        random_indices = np.random.choice(\n","            len(images), num_display, replace=False)\n","    else:\n","        random_indices = np.arange(num_display)\n","\n","    fig, axes = plt.subplots(\n","        nrows=n_rows, ncols=n_cols, figsize=(20, 4*n_rows))\n","    for i, ax in enumerate(axes.flatten()):\n","        if i >= n_images:  # Check if the index exceeds the available number of images\n","            break\n","        # Incase (Did PCA)\n","        index = random_indices[i]\n","        if len(images.shape) == 2:\n","            image = images[index].reshape((128, 128)).astype(int)\n","        else:\n","            image = images[index]\n","\n","        ax.imshow(image, cmap=cmap)\n","        ax.set_title(\"Target: {}\".format(target[index]))\n","\n","        # Add image index as text\n","        ax.text(0.5, -0.15, f'Image Index: {index}', transform=ax.transAxes,\n","                fontsize=10, ha='center')\n","\n","    plt.suptitle(f\"{title} (Displaying {num_display} Images)\",\n","                 fontsize=16, fontweight='bold')\n","\n","    fig.set_facecolor('white')\n","    plt.tight_layout() \n","    return plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:05.245345Z","iopub.status.busy":"2023-11-24T08:43:05.244279Z","iopub.status.idle":"2023-11-24T08:43:09.528634Z","shell.execute_reply":"2023-11-24T08:43:09.527379Z","shell.execute_reply.started":"2023-11-24T08:43:05.245308Z"},"trusted":true},"outputs":[],"source":["show_image(images=norm, \n","           target=target_norm,\n","           title='Sample of Normal images',\n","           num_display=16, \n","           num_cols=4, \n","           cmap='gray', \n","           random_mode=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:09.530485Z","iopub.status.busy":"2023-11-24T08:43:09.530084Z","iopub.status.idle":"2023-11-24T08:43:13.887877Z","shell.execute_reply":"2023-11-24T08:43:13.886534Z","shell.execute_reply.started":"2023-11-24T08:43:09.530451Z"},"trusted":true},"outputs":[],"source":["show_image(images=tb, \n","           target=target_tb,\n","           title='Sample of Tuberculosis images',\n","           num_display=16, \n","           num_cols=4, \n","           cmap='gray', \n","           random_mode=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:13.889709Z","iopub.status.busy":"2023-11-24T08:43:13.889349Z","iopub.status.idle":"2023-11-24T08:43:13.916271Z","shell.execute_reply":"2023-11-24T08:43:13.915029Z","shell.execute_reply.started":"2023-11-24T08:43:13.889679Z"},"trusted":true},"outputs":[],"source":["from skimage import img_as_float\n","from skimage import exposure"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:13.918451Z","iopub.status.busy":"2023-11-24T08:43:13.918092Z","iopub.status.idle":"2023-11-24T08:43:13.933538Z","shell.execute_reply":"2023-11-24T08:43:13.932417Z","shell.execute_reply.started":"2023-11-24T08:43:13.918421Z"},"trusted":true},"outputs":[],"source":["def plot_gray_scale_histogram(images, titles, bins=100):\n","    '''\n","    Plot Gray Scale Histograms of Images.\n","\n","    Parameters:\n","        - images (list): List of grayscale images to plot histograms for.\n","        - titles (list): List of titles for each histogram.\n","        - bins (int, optional): Number of bins for the histogram. Default is 100.\n","\n","    Returns:\n","        None\n","\n","    This function generates histograms for a list of grayscale images and displays them side by side. Each histogram is accompanied by its respective title.\n","\n","    The function does not return any values; it displays the histogram plots directly.\n","    '''\n","    # Display results\n","    fig, axes = plt.subplots(2, len(images), figsize=(20, 8))\n","\n","    for i, (title, image) in enumerate(zip(titles, images)):\n","        ax_img, ax_hist, ax_cdf, random_index = img_and_hist(\n","            image, axes[:, i], bins)\n","\n","        mean_value = np.mean(image)\n","        std_value = np.std(image)\n","        min_value = np.min(image)\n","        max_value = np.max(image)\n","\n","        ax_img.set_title('Random image of '+r'$\\bf{' + f'{title}'+'}$' +\n","                         f'\\nMean: {mean_value: .2f}, Std: {std_value: .2f}, Min: {min_value: .2f}, Max: {max_value: .2f}', fontsize=16)\n","        ax_img.text(0.5, -0.15, f'Image Index: {random_index}\\n(Display random image)', transform=ax_img.transAxes,\n","                    fontsize=10, ha='center')\n","\n","        y_min, y_max = ax_hist.get_ylim()\n","        ax_hist.set_title(\n","            'Distribution of pixel intensities of'+r'$\\bf{' + f'{title}'+'}$', fontsize=16)\n","        ax_hist.set_ylabel('Number of pixels')\n","        ax_hist.set_yticks(np.linspace(0, y_max, 5))\n","\n","        ax_cdf.set_ylabel('Fraction of total intensity')\n","        ax_cdf.set_yticks(np.linspace(0, 1, 5))\n","\n","    plt.suptitle('Gray scale Histogram: Distribution of intensity pixel',\n","                 fontsize=16, fontweight='bold')\n","    # Prevent overlap of y-axis labels\n","    fig.tight_layout()\n","    plt.show()\n","\n","\n","def img_and_hist(image_data, axes, bins=100):\n","    '''\n","    Plot an image along with its histogram and cumulative histogram.\n","\n","    Parameters:\n","        - image_data (ndarray): Grayscale image data as a numpy array.\n","        - axes (list): List of axes for displaying the image, histogram, and cumulative histogram.\n","        - bins (int): Number of bins for the histogram.\n","\n","    Returns:\n","        - ax_img, ax_hist, ax_cdf: Axes objects for image, histogram, and cumulative histogram.\n","\n","    This function displays an image along with its histogram and cumulative histogram. It takes the grayscale image data, a list of axes for plotting, and the number of bins for the histogram.\n","\n","    The function returns the axes objects for the image, histogram, and cumulative histogram.\n","    '''\n","\n","    '''\n","    Plot an image along with its histogram and cumulative histogram.\n","    '''\n","    image = img_as_float(image_data)\n","    ax_img, ax_hist = axes\n","    ax_cdf = ax_hist.twinx()\n","\n","    random_index = np.random.randint(0, len(image_data))\n","\n","    # Display image\n","    ax_img.imshow(image if image.shape[0] ==1 else image[random_index], cmap=plt.cm.gray)\n","    ax_img.set_axis_off()\n","\n","    # Display histogram\n","    ax_hist.hist(image.ravel(), bins=bins, histtype='step', color='black')\n","    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n","    ax_hist.set_xlabel('Pixel intensity')\n","    # ax_hist.set_xlim(0, 1)\n","    ax_hist.set_yticks([])\n","\n","    # Display cumulative distribution\n","    img_cdf, bins = exposure.cumulative_distribution(image, bins)\n","    ax_cdf.plot(bins, img_cdf, 'r')\n","    ax_cdf.set_yticks([])\n","\n","    return ax_img, ax_hist, ax_cdf, random_index"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:13.935818Z","iopub.status.busy":"2023-11-24T08:43:13.935134Z","iopub.status.idle":"2023-11-24T08:43:24.694222Z","shell.execute_reply":"2023-11-24T08:43:24.693027Z","shell.execute_reply.started":"2023-11-24T08:43:13.935758Z"},"trusted":true},"outputs":[],"source":["images = [norm, tb]\n","subsets = ['X_{Normal}', 'X_{Tuberculosis}']\n","# For visualization\n","plot_gray_scale_histogram(images=images, titles=subsets, bins=555)"]},{"cell_type":"markdown","metadata":{},"source":["\n","**Explaination:**\n","- **Pixel Intensity** 0 (completely black) to 255 (completely white)\n","- **X-axis** (Pixel intensity) represents the entire dataset's darkness (closer to 0) to brightness (closer to 255).\n","- **Slope Red line** refer to brighter or darker of all dataset\n","    - Right-Skewed Distribution, mean pixel intensity is higher, indicating brighter areas.\n","    - Left-Skewed Distribution, mean pixel intensity is lower, signifying darker areas.\n","So, \n","- **X_{Tuberulosis}** have a brighter or whiter areas\n","- **X_{Normal}** have a darker areas "]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 Data Augmentation (Class imbalance problem) (Optional):\n","Perform data augmentation techniques such as rotation, scaling, flipping, and adding noise to increase the diversity and size of your training dataset. This helps in improving the model's generalization ability.\n","### Objective\n","- solve class imbalance problem (700 TB images and 3500 normal images)\n","\n","### Not Recommended:\n","\n","1. **Reflection:**\n","    - Reflection in x-axis is discouraged due to potential confusion (e.g., 6 vs. 9).\n","    - Reflection in y-axis results in non-physiologic images, not recommended.\n","    - No method simulates differences between Posterior-Anterior (PA) and Anteroposterior (AP) chest X-rays.\n","2. **Severe Rotation:**\n","    - Avoid severe rotations (−90 to 90) as they may introduce unrealistic noise.\n","    - Slight rotations (−5 to 5) seen in practice can be helpful.\n","3. **Scaling:**\n","    - Large scaling (>×1) stretches the image, while small scaling (<×1) reduces its size.\n","    - Equal scaling in both axes is possible; scaling in only one axis is not recommended clinically.\n","4. **Shearing:**\n","    - Shearing is not recommended, producing clinically non-existent images.\n","\n","### Could Be Helpful/Acceptable:\n","\n","1. **Translation (Shifting):**\n","    - Shifting X-ray images can be useful to center lungs, enhancing detector robustness.\n","    - No clearly recommended translation range; considered acceptable clinically."]},{"cell_type":"markdown","metadata":{},"source":["### Summary of Clinical Perspective\n","| Augmentation Technique | Recommendation | Explanation |\n","| --- | --- | --- |\n","| Reflection (x-axis) | Not Recommended | Adds unnecessary noise and misleads the learning algorithm. |\n","| Reflection (y-axis) | Not Recommended | Leads to non-physiologic images and confounds learning. |\n","| Rotation | Could be helpful (within -5 to 5 degrees) | Slight rotations seen in clinical practice, but severe rotations are not recommended. |\n","| Scaling | Not Recommended | Large scaling stretches the image, small scaling reduces size. Equal scaling is possible, but x-axis or y-axis scaling is not recommended clinically. |\n","| Shearing | Not Recommended | Produces images that do not exist clinically. |\n","| Translation | Could be helpful/acceptable | Shifting X-ray images can be useful for better lung positioning, but no recommended range. |\n","Read more: https://www.frontiersin.org/articles/10.3389/fmed.2021.629134/full"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:24.695802Z","iopub.status.busy":"2023-11-24T08:43:24.695467Z","iopub.status.idle":"2023-11-24T08:43:38.167951Z","shell.execute_reply":"2023-11-24T08:43:38.166714Z","shell.execute_reply.started":"2023-11-24T08:43:24.695772Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:38.170421Z","iopub.status.busy":"2023-11-24T08:43:38.169751Z","iopub.status.idle":"2023-11-24T08:43:38.181252Z","shell.execute_reply":"2023-11-24T08:43:38.179863Z","shell.execute_reply.started":"2023-11-24T08:43:38.170382Z"},"trusted":true},"outputs":[],"source":["def x_ray_image_augmentation(images, targets, sample_count):\n","    '''\n","    :Parameters\n","        df (ndarray (n,)): Input data as a numpy array.\n","        target (ndarray (n,)): Target data as a numpy array.\n","        sample_count (integer64): Number of data needed.\n","    '''\n","\n","    # Randomize the data augmentation parameters\n","    rotation_range = random.randint(-5, 5)\n","    # width_shift_range = random.uniform(0.0, 0.05)      Not reccomend\n","    # height_shift_range = random.uniform(0.0, 0.05)     Not reccomend\n","    # shear_range = random.uniform(0.0, 0.1)             Not reccomend\n","    zoom_range = random.uniform(0.0, 0.1)\n","    fill_mode = random.choice(['nearest',\n","                               'constant',\n","                               #    'reflect',               Not reccomend\n","                               'wrap'])\n","\n","    # Create the ImageDataGenerator with randomized parameters\n","    datagen = ImageDataGenerator(\n","        rotation_range=rotation_range,\n","        # width_shift_range=width_shift_range,             Not reccomend\n","        # height_shift_range=height_shift_range,           Not reccomend\n","        # shear_range=shear_range,                         Not reccomend\n","        zoom_range=zoom_range,\n","        fill_mode=fill_mode\n","    )\n","    # Load and augment the data\n","\n","    augmented_data_generator = datagen.flow(images, targets, batch_size=1)\n","    batch_images, batch_labels = augmented_data_generator.next()\n","    for _ in tqdm(range(sample_count), desc='augmenting...'):\n","        batch_images, batch_labels = augmented_data_generator.next()\n","        try:\n","            combined_images = np.concatenate(\n","                (combined_images, batch_images), axis=0)\n","            combined_labels = np.concatenate(\n","                (combined_labels, batch_labels), axis=0)\n","        except NameError:\n","            combined_images = np.concatenate((images, batch_images), axis=0)\n","            combined_labels = np.concatenate((targets, batch_labels), axis=0)\n","    '''\n","    Returns:\n","        combined_images (ndarray (n,)) : generated images\n","        combined_labels (ndarray (n,)) : generated labels\n","    '''\n","\n","    return combined_images, combined_labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:43:38.18339Z","iopub.status.busy":"2023-11-24T08:43:38.182913Z","iopub.status.idle":"2023-11-24T08:43:38.768682Z","shell.execute_reply":"2023-11-24T08:43:38.7673Z","shell.execute_reply.started":"2023-11-24T08:43:38.183349Z"},"trusted":true},"outputs":[],"source":["# Scaled the dataset before do Augmentation \n","norm = norm/255\n","tb = tb/255"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:44:53.899904Z","iopub.status.busy":"2023-11-24T08:44:53.899501Z","iopub.status.idle":"2023-11-24T08:44:57.552437Z","shell.execute_reply":"2023-11-24T08:44:57.551306Z","shell.execute_reply.started":"2023-11-24T08:44:53.899876Z"},"trusted":true},"outputs":[],"source":["show_image(images=tb,\n","           target=target_tb,\n","           title='After scaled Tuberculosis data', \n","           num_display=16, \n","           num_cols=8, \n","           cmap='gray', \n","           random_mode=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T08:45:14.580332Z","iopub.status.busy":"2023-11-24T08:45:14.579914Z","iopub.status.idle":"2023-11-24T09:00:12.530337Z","shell.execute_reply":"2023-11-24T09:00:12.528718Z","shell.execute_reply.started":"2023-11-24T08:45:14.580304Z"},"trusted":true},"outputs":[],"source":["tb_augmented, target_tb_augmented = x_ray_image_augmentation(images=tb, \n","                                                             targets=target_tb, \n","                                                             sample_count=2800)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:00:30.917766Z","iopub.status.busy":"2023-11-24T09:00:30.917308Z","iopub.status.idle":"2023-11-24T09:00:30.926031Z","shell.execute_reply":"2023-11-24T09:00:30.924796Z","shell.execute_reply.started":"2023-11-24T09:00:30.917729Z"},"trusted":true},"outputs":[],"source":["tb_augmented.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:00:31.612104Z","iopub.status.busy":"2023-11-24T09:00:31.611704Z","iopub.status.idle":"2023-11-24T09:00:31.618144Z","shell.execute_reply":"2023-11-24T09:00:31.617059Z","shell.execute_reply.started":"2023-11-24T09:00:31.612075Z"},"trusted":true},"outputs":[],"source":["print(\n","    f'Before {tb.shape[0]}\\nAfter Data Augmentation {tb_augmented.shape[0]}\\n+{tb_augmented.shape[0]-tb.shape[0]} Training examples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:00:34.372919Z","iopub.status.busy":"2023-11-24T09:00:34.372188Z","iopub.status.idle":"2023-11-24T09:00:48.023491Z","shell.execute_reply":"2023-11-24T09:00:48.021783Z","shell.execute_reply.started":"2023-11-24T09:00:34.372884Z"},"trusted":true},"outputs":[],"source":["# Start index since 700 to last to observe to new training images\n","show_image(images=tb_augmented[700:-1],\n","           target=target_tb_augmented[700:-1],\n","           title='Augmented Tuberculosis data', \n","           num_display=64, \n","           num_cols=8, \n","           cmap='gray', \n","           random_mode=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Compare Distribution (Tuberculosis Original Dataset vs Tuberculosis + Augmentation)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:21.710425Z","iopub.status.busy":"2023-11-24T09:01:21.709982Z","iopub.status.idle":"2023-11-24T09:01:32.008753Z","shell.execute_reply":"2023-11-24T09:01:32.007366Z","shell.execute_reply.started":"2023-11-24T09:01:21.710391Z"},"trusted":true},"outputs":[],"source":["images_2 = [tb, tb_augmented]\n","subsets_2 = ['X_{Tuberculosis}', 'X_{TuberculosisAugmentation}']\n","# For visualization\n","plot_gray_scale_histogram(images=images_2, titles=subsets_2, bins=555)"]},{"cell_type":"markdown","metadata":{},"source":["**Explanation**:\n","\n","Both distributions are similar, as indicated by the red lines and statistical descriptors."]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 Data Preparation\n","prepare data to facilitate further processing and training. We will create explicit variables that are easy to call and work with.\n","\n","- **X**; representing 7,000 samples, each with a size of 128x128 pixels. contains\n","    - 3500 samples will be the normal images \n","    - 3500 samples will be the TB images \n","\n","- **y**; represents the binary classification labels, where \n","    - 0 represents a normal image\n","    - 1 represents a TB image. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:39.776461Z","iopub.status.busy":"2023-11-24T09:01:39.77606Z","iopub.status.idle":"2023-11-24T09:01:41.007491Z","shell.execute_reply":"2023-11-24T09:01:41.006367Z","shell.execute_reply.started":"2023-11-24T09:01:39.776429Z"},"trusted":true},"outputs":[],"source":["# Concatenate df_norm and df_tb along the first axis (samples)\n","X = np.concatenate((norm, tb_augmented), axis=0)\n","\n","# Concatenate target_norm and target_tb\n","y = np.concatenate((target_norm,  target_tb_augmented), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:41.610688Z","iopub.status.busy":"2023-11-24T09:01:41.610293Z","iopub.status.idle":"2023-11-24T09:01:41.617627Z","shell.execute_reply":"2023-11-24T09:01:41.616647Z","shell.execute_reply.started":"2023-11-24T09:01:41.610658Z"},"trusted":true},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:42.058145Z","iopub.status.busy":"2023-11-24T09:01:42.057443Z","iopub.status.idle":"2023-11-24T09:01:42.064453Z","shell.execute_reply":"2023-11-24T09:01:42.063283Z","shell.execute_reply.started":"2023-11-24T09:01:42.05811Z"},"trusted":true},"outputs":[],"source":["y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:42.551832Z","iopub.status.busy":"2023-11-24T09:01:42.551406Z","iopub.status.idle":"2023-11-24T09:01:42.557379Z","shell.execute_reply":"2023-11-24T09:01:42.556015Z","shell.execute_reply.started":"2023-11-24T09:01:42.551799Z"},"trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:43.006444Z","iopub.status.busy":"2023-11-24T09:01:43.006032Z","iopub.status.idle":"2023-11-24T09:01:43.031294Z","shell.execute_reply":"2023-11-24T09:01:43.03029Z","shell.execute_reply.started":"2023-11-24T09:01:43.006411Z"},"trusted":true},"outputs":[],"source":["y_df = pd.DataFrame(y)\n","y_df.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["Balanced dataset!"]},{"cell_type":"markdown","metadata":{},"source":["## 1.5 Image Enhancement \n","Image Pre-processing for Chest X-ray\n","\n","<u>used</u> to improve their visual quality and enhance the details within the images.\n","\n","Aim: \n","- Improve the visibility of details in the image\n","- Enhance the contrast of the image"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5.1 Contrast Stretching:\n","- Objective: Enhance the visibility of details and improve contrast in an image.\n","- Goal: Increase the dynamic range of pixel values by stretching the intensity values to a desired range.\n","- Pros:\n","    - Simple and easy to implement.\n","    - Enhances the contrast and visibility of details in an image.\n","- Cons:\n","    - It may not be effective for images with extremely low or high contrast.\n","    - It can amplify noise or artifacts in the image."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:46.829192Z","iopub.status.busy":"2023-11-24T09:01:46.828043Z","iopub.status.idle":"2023-11-24T09:01:46.833648Z","shell.execute_reply":"2023-11-24T09:01:46.832614Z","shell.execute_reply.started":"2023-11-24T09:01:46.829151Z"},"trusted":true},"outputs":[],"source":["from skimage import exposure"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:47.30804Z","iopub.status.busy":"2023-11-24T09:01:47.307248Z","iopub.status.idle":"2023-11-24T09:01:47.314122Z","shell.execute_reply":"2023-11-24T09:01:47.313099Z","shell.execute_reply.started":"2023-11-24T09:01:47.307995Z"},"trusted":true},"outputs":[],"source":["def contrast_stretching(X):\n","    p2, p98 = np.percentile(X, (2, 98))\n","    stretched_img = exposure.rescale_intensity(X, in_range=(p2, p98))\n","    return stretched_img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:01:47.881363Z","iopub.status.busy":"2023-11-24T09:01:47.880954Z","iopub.status.idle":"2023-11-24T09:01:52.085767Z","shell.execute_reply":"2023-11-24T09:01:52.084675Z","shell.execute_reply.started":"2023-11-24T09:01:47.881332Z"},"trusted":true},"outputs":[],"source":["X_stretched = contrast_stretching(X[0:16])\n","show_image(images=X_stretched,\n","           target=y,\n","           title='Contrast Stretching images', \n","           num_display=16, \n","           num_cols=4, \n","           cmap='gray', \n","           random_mode=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5.2 Histogram Equalization:\n","- Objective: Enhance the global contrast and improve the overall brightness distribution in an image.\n","- Goal: Adjust the pixel intensity distribution to achieve a uniform histogram.\n","- Pros:\n","    - Effective in improving the global contrast of an image.\n","    - Enhances the visibility of details in both dark and bright regions.\n","- Cons:\n","    - May result in over-enhancement or unnatural appearance if applied to certain images.\n","    - Does not consider local image characteristics."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:08.175679Z","iopub.status.busy":"2023-11-24T09:02:08.175264Z","iopub.status.idle":"2023-11-24T09:02:08.181448Z","shell.execute_reply":"2023-11-24T09:02:08.179943Z","shell.execute_reply.started":"2023-11-24T09:02:08.17565Z"},"trusted":true},"outputs":[],"source":["def equalization(X):\n","    eq_img = exposure.equalize_hist(X)\n","    return eq_img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:17.753326Z","iopub.status.busy":"2023-11-24T09:02:17.752867Z","iopub.status.idle":"2023-11-24T09:02:22.446727Z","shell.execute_reply":"2023-11-24T09:02:22.445512Z","shell.execute_reply.started":"2023-11-24T09:02:17.753297Z"},"trusted":true},"outputs":[],"source":["X_eq = equalization(X[0:16])\n","show_image(images=X_eq,\n","           target=y,\n","           title='Histogram Equalization images', \n","           num_display=16, \n","           num_cols=4, \n","           cmap='gray', \n","           random_mode=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5.3 Adaptive Histogram Equalization:\n","- Objective: Enhance the contrast while preserving local details in an image.\n","- Goal: Apply histogram equalization locally to different regions of an image.\n","- Pros:\n","    - Enhances the contrast of images with varying local characteristics.\n","    - Preserves local details and avoids over-enhancement in homogeneous regions.\n","- Cons:\n","    - Can amplify noise or artifacts in regions with high frequency variations.\n","    - Requires additional computational resources compared to global histogram equalization.\n"]},{"cell_type":"markdown","metadata":{},"source":["Usage example: \n","``` python\n","def adaptive_equalization(X, clip_limit):\n","    '''\n","    :Parameters:\n","    X (ndarray (n,)): Input data as a numpy array, X.shape should be (128, 128, 1)\n","    clip_limit (int or float): used to control the contrast\n","    '''\n","    # Normalize the input image to the range of -1 to 1 and perform adaptive histogram equalization\n","    X_normalized = (X - np.min(X)) / (np.max(X) - np.min(X))\n","    X_normalized = (X_normalized * 2) - 1  # Scale to -1 to 1\n","    adapteq_img = exposure.equalize_adapthist(\n","        X_normalized, clip_limit=clip_limit)\n","\n","    return adapteq_img\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["> Noted: I added '**batch_size**' parameter, Because of computational and memory constraints."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:22.449356Z","iopub.status.busy":"2023-11-24T09:02:22.448818Z","iopub.status.idle":"2023-11-24T09:02:22.460011Z","shell.execute_reply":"2023-11-24T09:02:22.458557Z","shell.execute_reply.started":"2023-11-24T09:02:22.449323Z"},"trusted":true},"outputs":[],"source":["def adaptive_equalization(X, clip_limit, batch_size=None):\n","    '''\n","    :Parameters:\n","    X (ndarray (n,)): Input data as a numpy array, X.shape should be (128, 128, 1)\n","    clip_limit (int or float): used to control the contrast     \n","    batch_size (int): Size of the batch to process. If None, the entire input is processed at once.\n","    '''\n","    if batch_size is None:\n","        # Normalize the input image to the range of -1 to 1 and perform adaptive histogram equalization\n","        X_normalized = (X - np.min(X)) / (np.max(X) - np.min(X))\n","        X_normalized = (X_normalized * 2) - 1  # Scale to -1 to 1\n","        adapteq_img = exposure.equalize_adapthist(\n","            X_normalized, clip_limit=clip_limit)\n","    else:\n","        # Process data in batches\n","        num_batches = int(np.ceil(len(X) / batch_size))\n","        batch_results = []\n","\n","        for i in tqdm(range(num_batches),desc='process...'):\n","            start_idx = i * batch_size\n","            end_idx = min((i + 1) * batch_size, len(X))\n","\n","            # Normalize the input image to the range of -1 to 1 and perform adaptive histogram equalization for each batch\n","            X_normalized = (X[start_idx:end_idx] - np.min(X[start_idx:end_idx])) / \\\n","                (np.max(X[start_idx:end_idx]) - np.min(X[start_idx:end_idx]))\n","            X_normalized = (X_normalized * 2) - 1  # Scale to -1 to 1\n","            adapteq_img_batch = exposure.equalize_adapthist(\n","                X_normalized, clip_limit=clip_limit)\n","            batch_results.append(adapteq_img_batch)\n","\n","        # Concatenate the batch results\n","        adapteq_img = np.concatenate(batch_results, axis=0)\n","\n","    return adapteq_img"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:22.616431Z","iopub.status.busy":"2023-11-24T09:02:22.616049Z","iopub.status.idle":"2023-11-24T09:02:28.537281Z","shell.execute_reply":"2023-11-24T09:02:28.536383Z","shell.execute_reply.started":"2023-11-24T09:02:22.616403Z"},"trusted":true},"outputs":[],"source":["X_adapteq = adaptive_equalization(X[0:64],0.03)\n","show_image(images=X_adapteq,\n","            target=y,\n","            title='Adaptive Histogram Equalization', \n","            num_display=16, \n","            num_cols=4, \n","            cmap='gray', \n","            random_mode=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.5.4 Summary of each image enhancement "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:42.991737Z","iopub.status.busy":"2023-11-24T09:02:42.99133Z","iopub.status.idle":"2023-11-24T09:02:42.999418Z","shell.execute_reply":"2023-11-24T09:02:42.998231Z","shell.execute_reply.started":"2023-11-24T09:02:42.991698Z"},"trusted":true},"outputs":[],"source":["sample = X[0,:,:,:]\n","sample.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:44.164239Z","iopub.status.busy":"2023-11-24T09:02:44.163819Z","iopub.status.idle":"2023-11-24T09:02:44.212646Z","shell.execute_reply":"2023-11-24T09:02:44.211491Z","shell.execute_reply.started":"2023-11-24T09:02:44.164198Z"},"trusted":true},"outputs":[],"source":["# 1. Load a normal image\n","img = sample\n","\n","# 2. Contrast stretching\n","img_cs = contrast_stretching(img)\n","\n","# 3. Histogram Equalization\n","img_eq = equalization(img)\n","\n","# 4. Adaptive Equalization\n","img_adapteq = adaptive_equalization(img, 0.03)\n","\n","methods = ['Low contrast image', 'Contrast stretching',\n","           'Histogram equalization', 'Adaptive equalization']\n","images_3 = [img, img_cs, img_eq, img_adapteq]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:45.913342Z","iopub.status.busy":"2023-11-24T09:02:45.912642Z","iopub.status.idle":"2023-11-24T09:02:45.922425Z","shell.execute_reply":"2023-11-24T09:02:45.921114Z","shell.execute_reply.started":"2023-11-24T09:02:45.913297Z"},"trusted":true},"outputs":[],"source":["def img_and_hist_single_img(image, axes, bins=100):\n","    '''\n","    Plot an image along with its histogram and cumulative histogram.\n","    '''\n","    image = img_as_float(image)\n","    ax_img, ax_hist = axes\n","    ax_cdf = ax_hist.twinx()\n","\n","    # Display image\n","    ax_img.imshow(image, cmap=plt.cm.gray)\n","    ax_img.set_axis_off()\n","\n","    # Display histogram\n","    ax_hist.hist(image.ravel(), bins=bins, histtype='step', color='black')\n","    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n","    ax_hist.set_xlabel('Pixel intensity')\n","    # ax_hist.set_xlim(0, 1)\n","    ax_hist.set_yticks([])\n","\n","    # Display cumulative distribution\n","    img_cdf, bins = exposure.cumulative_distribution(image, bins)\n","    ax_cdf.plot(bins, img_cdf, 'r')\n","    ax_cdf.set_yticks([])\n","\n","    return ax_img, ax_hist, ax_cdf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:02:47.477923Z","iopub.status.busy":"2023-11-24T09:02:47.477308Z","iopub.status.idle":"2023-11-24T09:02:49.777549Z","shell.execute_reply":"2023-11-24T09:02:49.776578Z","shell.execute_reply.started":"2023-11-24T09:02:47.47788Z"},"trusted":true},"outputs":[],"source":["# Display results\n","fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n","\n","for i, (method, image) in enumerate(zip(methods, images_3)):\n","    ax_img, ax_hist, ax_cdf = img_and_hist_single_img(image, axes[:, i], bins=555)\n","\n","    mean_value = np.mean(image)\n","    std_value = np.std(image)\n","    min_value = np.min(image)\n","    max_value = np.max(image)\n","\n","    ax_img.set_title(\n","        r'$\\bf{' + f'{method}'+'}$'+f'\\nMean: {mean_value: .2f}, Std: {std_value: .2f}, Min: {min_value: .2f}, Max: {max_value: .2f}', fontsize=12)\n","\n","    y_min, y_max = ax_hist.get_ylim()\n","    ax_hist.set_title(\n","        'Distribution of pixel intensitie', fontsize=12, fontweight='bold')\n","    ax_hist.set_ylabel('Number of pixels')\n","    ax_hist.set_yticks(np.linspace(0, y_max, 5))\n","\n","    ax_cdf.set_ylabel('Fraction of total intensity')\n","    ax_cdf.set_yticks(np.linspace(0, 1, 5))\n","\n","plt.suptitle('Summary image enhancement techniques',\n","             fontsize=16, fontweight='bold')\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Explanation\n","\n","**Overview of plot**: aims to improve the contrast \"Low contrast image.\"\n","\n","**Red line**: (Black/White balances) A steeper curve indicates an increased Black/White intensity\n","\n","| Method | Range | Pixel Intensity Distribution | Image | Conclusion | \n","| --- | --- | --- | --- | --- |\n","| Low Contrast | [0, 255] | Original image | Original image | Original image |\n","| Contrast Stretching | [0, 1] | Similar distribution to original image | Similar to original image | Dataset normalized, Distribution similar to original image |\n","| Histogram Equalization | [0, 1] | New distribution | Similar to original image, But a little bit higher contrast |  Dataset normalized, Black/White is balanced |\n","| Adaptive Histogram Equalization | [0, 1] | New distribution | High contrast, Different local contrast in different regions of the image compared to the original image | Dataset normalized, Desirable contrast |\n","\n","### How to consider \n","<u>Answer</u> In fact we need to experiment <b>all image enhancement techniques</b> for Training in CNN, Which one give us a better Performance?\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. CNN:\n","\n","## Deep Learning - Convolutional Neural Networks\n","Convolutional Neural Networks (CNNs) are a class of deep learning algorithms specifically designed for image processing, pattern recognition, and computer vision tasks. They have revolutionized the field of computer vision and have achieved state-of-the-art results in various image-related applications, such as object detection, image classification, segmentation, and more.\n","\n","CNN are designed to perform tasks on the 2-D image, In the context of binary classification for TB detection, we can customize the output layer of the CNN to have a single unit, which will classify the input as either TB or normal.\n","\n","![Alt Text](https://dwbi1.files.wordpress.com/2021/07/fig-1-cnn-architecture.jpg)\n","\n","Image Source: https://dwbi1.wordpress.com/2021/07/04/what-is-convolutional-neural-network-cnn/\n","\n","![Alt Text](https://www.mdpi.com/electronics/electronics-11-01775/article_deploy/html/images/electronics-11-01775-g001.png)\n","Image Source: https://www.mdpi.com/2079-9292/11/11/1775"]},{"cell_type":"markdown","metadata":{},"source":["### Why CNN?\n","CNNs are specifically suitable for image data:\n","- ANN (Artificial Neural Network) - Tabular data/ 1D -  used for tasks such as regression, binary classification, and multi-class classification on structured data\n","- RNN (Recurrent Neural Network) - Tabular data/ 1D (Time Series), designed for handling sequential data\n","- CNN (Convolutional Neural Network) - Image\n","\n","### Use Cases \n","#### ANN (Artificial Neural Network)\n","\n","Data Type: Tabular data/1D\n","Typical Use Cases:\n","\n","- Regression\n","- Binary Classification\n","- Multi-class Classification (structured data)\n","\n","#### RNN (Recurrent Neural Network)\n","\n","Data Type: Tabular data/1D (Time Series)\n","Typical Use Cases:\n","\n","- Time Series Forecasting\n","- Natural Language Processing\n","- Speech Recognition\n","\n","#### CNN (Convolutional Neural Network)\n","\n","Data Type: Image\n","Typical Use Cases:\n","\n","- Image Classification\n","- Object Detection\n","- Image Segmentation\n","- Image Generation"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 How CNNs Work:\n","\n","Convolutional Neural Networks (CNNs) designed for image and visual data processing.\n","\n","Training Process:\n","- Forward propagation\n","    - Receives the input images in a variable (say X) and applies a series of linear transformations through hidden layers using activation functions. until the **final output** layer produces predictions for the given input.\n","- Backward propagation\n","    - Randomly initialized the weights, biases and Adjust/Update parameters (weights and biases) during this step to minimize the Error/Loss function (**Gradient Optimization**) and improve the model's performance. \n","\n","CNNs Learning: Detecting Features in Images\n","- Convolutional Layers: ability to learn and detect the entire image and patterns, edges, and features. \n","- Activation Functions: each layer applies an activation function, input images and calculated by activation function transform to the output of each layers.\n","- Pooling Layers: retain the most important information while reducing the computational complexity.\n","\n","Summary using a Convolution or Pooling layer we reduce the dimensions of the input image of dimensions\n","\n","Read more **Demystifying the Mathematics Behind Convolutional Neural Networks (CNNs)**: https://www.analyticsvidhya.com/blog/2020/02/mathematics-behind-convolutional-neural-network/"]},{"cell_type":"markdown","metadata":{},"source":["## 2.2 Layers\n","\n","### 2.1.1 Convolutional Layers:\n","These layers perform the core operation in CNNs - convolution. Convolution involves sliding a set of small filters (also known as kernels) across the input image and computing dot products to create feature maps. Each filter specializes in detecting certain features like edges, corners, textures, etc.\n","\n","For simplicity, let’s stick with grayscale images as we try to understand how CNNs work.\n","\n","\n","![image.png](https://editor.analyticsvidhya.com/uploads/750710_QS1ArBEUJjjySXhE.png)\n","Image Source: https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/\n","\n"," filter/kernel(3×3 matrix) and apply it to the input image to get the convolved feature. This convolved feature is passed on to the next layer.\n"," Each filter specializes in detecting certain features like edges, corners, textures, etc.\n"]},{"cell_type":"markdown","metadata":{},"source":["This layer learn to detect edges\n","\n","![image-2.png](https://editor.analyticsvidhya.com/uploads/52794neural-networks-layers-visualization.jpg)\n","\n","Image Source: https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1.2 Pooling Layers:\n","\n","![image.png](https://1299806939-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LvMRntv-nKvtl7WOpCz%2F-LvMRp9FltcwEeVxPYFs%2F-LvMRrqrlhiBpH5LD-_p%2FPooling_Simple_max.png?generation=1575572710691611&alt=media)\n","\n","Image Source: https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/pooling_layer\n","\n","\n","Pooling layers are **used to summarize the important features extracted from the previous convolutional layers. These layers reduce the spatial dimensions of the feature maps, typically reducing them by half.**\n","\n","Pooling layers reduce the spatial dimensions of feature maps while <u>preserving</u> essential information. Each pooling region is transformed into a single output value, representing the presence of a particular feature in that region.\n","\n","Objective: \n","- Reduces the spatial dimensions of the feature maps, making the model more efficient and reducing overfitting.\n","- Maintaining essential information while improving computational efficiency.\n","\n","Common pooling techniques are **max pooling**, which selects the maximum value in a local region, and **average pooling**\n","\n","### 2.1.3 Dropout Layer\n","Random drop some Neural, \n","Dropout is a regularization technique used to prevent overfitting in deep learning models. It involves randomly deactivating a fraction of neurons during training.\n","**Dropout**:  will prevent our network from overfitting thus helping our network generalize better. How much dropout should we add after each layer?\n","0%, 10%, 20%, 30%, 40%, 50%, 60%, or 70%\n","\n","How it works:\n","\n","Installing the dropout layer, a so-called dropout probability must also be specified. This determines how many of the nodes/units in the layer will be set equal to 0. If we have an input layer with ten input values, a dropout probability of 10% means that one random input will be set equal to zero in each training pass. If instead, it is a hidden layer, the same logic is applied to the hidden nodes/units. So a dropout probability of 10% means that 10% of the nodes/units will not be used in each run.\n","\n","Dropout layer is often <u>used</u> after a **Fully Connected (Dense) Layers**\n","> Noted 1 **Dropout**:  will prevent our network from overfitting thus helping our network generalize better. How much dropout should we add after each layer?\n","0%, 10%, 20%, 30%, 40%, 50%, 60%, or 70%\n","\n","\n","### 2.1.4 Flatten Layer\n","The Flatten layer reshapes the output from previous layers into a 1D vector. It is used to convert the multidimensional feature maps into a flat feature vector so that it can be processed by fully connected layers.\n","**Enables the utilization of fully connected layers to make predictions or decisions based on the learned features.**\n","\n","Pros:\n","- Enables the model to process the learned features from convolutional layers in fully connected layers.\n","- Simplifies the representation of data, making it easier for the subsequent layers to interpret and learn from the features.\n","- Allows the model to be compatible with architectures that require 1D inputs, such as traditional fully connected neural networks.\n","Cons:\n","- might losing some locality information.\n","\n","### 2.1.5 Fully Connected (Dense) Layers\n","Fully connected layers consist of a fixed number of neurons connected to every neuron in the previous layer. They help combine learned features and make the final predictions. ReLU activation functions are commonly used for hidden layers, introducing non-linearity. For binary classification tasks like TB detection, a sigmoid activation function is used in the output layer to produce a probability score between 0 and 1 for classifying an image as TB or normal.\n","**Combining the learned features from previous layers and making final predictions in a neural network.**\n","\n","### 2.1.6 Output Layers \n","The Output Layer is the final layer of a neural network that produces the model's predictions. In binary classification tasks like Tuberculosis (TB) detection, the output layer typically consists of a single neuron with a sigmoid activation function.\n","\n","Objective:\n","output layer is to generate a probability score between 0 and 1, representing the likelihood of the input belonging to a particular class. In TB detection, the output layer aims to classify the input chest X-ray image as either TB positive or normal based on the learned features from the previous layers.\n","\n","Outcomes:\n","**a probability score close to 0 for normal images and close to 1 for TB images**, allowing the model to classify the input X-ray images accurately.\n","By applying an appropriate threshold to the output probability score, we can make the final decision on whether an X-ray image is classified as TB positive or normal."]},{"cell_type":"markdown","metadata":{},"source":["## 2.3 Hyperparameters in both **convolutional** and **pooling** layers\n","\n","### 2.3.1 Padding\n","Padding is the process of adding extra border pixels around the input image before applying convolutions.\n","\n","**Purpose**:\n","- preserve the spatial dimensions of the feature maps, especially at the edges, which can be lost during convolutional operations.\n","- helps avoid the reduction in the feature map's size, which can occur when using a large kernel or multiple layers of convolutions.\n","- Padding involves adding zero-value pixels around the input image. This helps to <u>preserve</u> valuable information during convolution and ensures that the feature map doesn't become excessively small.\n","\n","**Problem**: When Convolution → smaller output\n","\n","**Solution**: +Padding\n","\n","**When to use**: \n","\n","Try using padding = same when you feel the border’s of the image might be important, Because when Convolution → smaller output\n","**Type of Padding**:\n","\n","![0rs9l.gif](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*jSYji1D43vJWZZ_5)\n","\n","Image Source: https://sarathpanat.medium.com/all-about-convolutional-neural-network-cnn-6ccce6738958\n","\n","\n","1. Padding `'valid'` is the 1st figure.\n","    - means no padding at all. Just leave your data the same it was.    \n","2. Padding `'same'` is the 3rd figure.\n","    - The output is the same size. It is called SAME because for a convolution with a stride=1, (or for pooling) it should produce output of the same size as the input. \n","\n","\n","### 2.3.2 Kernel size\n","refers small filter applied during the convolution operation.\n","Common kernel sizes are (3x3), (5x5), and (7x7), but other sizes\n","\n","Purpose:\n","-  captures more contextual information\n","\n","### 2.3.3 Stride \n","skip scanning pixel\n","controls the step size of the filter as it scans the input image.\n","In both **convolutional** and **pooling** layers:\n","1. Convolutional Stride: A stride of 1 means the **filter** moves one **pixel** at a time\n","2. Pooling Stride: A stride of 1 means the **window** moves one **pixel** at a time\n","\n","\n","\n","\n","**Purpose**:\n","- faster computation and downsampling of the data.\n","\n","**When to use**: \n","- Stride = 1: Input data is small, and preserving spatial information is important.\n","- Stride > 1: Input data is large, help reduce computational cost and memory usage, But lead to **information loss**\n","\n","![0_iqNdZWyNeCr5tCkc.gif](https://miro.medium.com/v2/resize:fit:640/format:webp/0*wW8SEzhhhTDN30Ow)\n","\n","Image Source: https://sarathpanat.medium.com/all-about-convolutional-neural-network-cnn-6ccce6738958\n","\n","\n","> Noted 2 **Stride**: Stride just means the amount a filter moves during a covolution operation. So, a stride of 1 means that the filter will slide 1 pixel after each covolution operation as shown in this animation.\n","\n","\n","> Noted 3 **In practice**: Stride is often set based on experimentation and model performance on the validation set."]},{"cell_type":"markdown","metadata":{},"source":["## 2.4 Activation Functions\n","### 2.4.1 ReLU Activation function \n","The Rectified Linear Activation function (ReLU) is specifically used as a non-linear activation function, as opposed to other non-linear functions such as Sigmoid because it has been empirically observed that CNNs using ReLU are faster to train than their counterparts. ReLU is performed after every convolutional layer\n","\n","The ReLU activation function is a one-to-one mathematical operation:\n","#### Why this Activations?  \n","Using non-linear activation functions like ReLU is crucial because real-world data and patterns are often complex and non-linear. If we only use linear activation functions, the neural network can only learn linear relationships, limiting its ability to capture more intricate patterns and reducing its performance. better performance in tasks like image classification, object detection, and more.\n","\n","#### Output of this Activations?  \n","1. If the input is greater than or equal to zero, the output is equal to the input itself (Output = Input).\n","2. If the input is less than zero, the output is zero (Output = 0).\n","\n","In mathematical terms, the ReLU function can be defined as follows:\n","\n","$$ReLU(x) = max(0, x) \\tag{1}$$\n","\n","\n","\n","### 2.4.2 Sigmoid Activation function \n","used in the output layer of neural networks for Binary classification tasks. The Sigmoid activation function is a non-linear activation commonly used in neural networks. It maps any input value to a range between 0 and 1, allowing it to interpret the input as probabilities.\n","\n","The Sigmoid function can be written:\n","$$sigmoid(x) = \\frac{1}{1+{e^{-x} }} \\tag{2}$$\n","Where \"x\" is the input value, and \"e\" represents the exponential function.\n","\n","\n","\n","#### Why this Activations?  \n","- <u>used</u> in the output layer of neural networks for binary classification tasks, where the goal is to classify data into two classes (e.g., yes or no, true or false). \n","- <u>converts</u> the model's raw output into a probability value\n","\n","#### Output of this Activations?  \n","- <u>transforms</u> input values into probabilities between 0 and 1, making it suitable for binary classification tasks that require a probability-based decision."]},{"cell_type":"markdown","metadata":{},"source":["Table representing the architecture of the CNN model \n","\n","| Layer (Type)   | Number of Filters | Padding | Activation | Out of Activation Shape | Total Parameters            |\n","|-----------------|-------------------|---------|------------|--------------------------|-----------------------------|\n","| Input           | -                 | -       | -          | (128, 128, 1)            | 0                           |\n","| Conv2D          | 32 * unit_size_rate| Same    | ReLU       | (128, 128, 32)           | 320 * unit_size_rate        |\n","| MaxPooling2D    | -                 | Same    | -          | (64, 64, 32)             | 0                           |\n","| Conv2D          | 64 * unit_size_rate| Same    | ReLU       | (64, 64, 64)             | 18,496 * unit_size_rate     |\n","| MaxPooling2D    | -                 | Same    | -          | (32, 32, 64)             | 0                           |\n","| Conv2D          | 128 * unit_size_rate| Same    | ReLU       | (32, 32, 128)            | 73,856 * unit_size_rate     |\n","| MaxPooling2D    | -                 | Same    | -          | (16, 16, 128)            | 0                           |\n","| Flatten         | -                 | -       | -          | (32768)                  | 0                           |\n","| Dense           | 128 * unit_size_rate| -      | ReLU       | (128)                    | 4,194,560 * unit_size_rate |\n","| Dropout         | -                 | -       | -          | (128)                    | 0                           |\n","| Dense           | 1                 | -       | Sigmoid    | (1)                      | 129                         |\n","\n","Total Params: 4,287,361 * unit_size_rate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:03:01.994419Z","iopub.status.busy":"2023-11-24T09:03:01.994032Z","iopub.status.idle":"2023-11-24T09:03:02.002291Z","shell.execute_reply":"2023-11-24T09:03:02.000892Z","shell.execute_reply.started":"2023-11-24T09:03:01.99439Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, Dense"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:28:56.4532Z","iopub.status.busy":"2023-11-24T10:28:56.452749Z","iopub.status.idle":"2023-11-24T10:28:56.474499Z","shell.execute_reply":"2023-11-24T10:28:56.4735Z","shell.execute_reply.started":"2023-11-24T10:28:56.453167Z"},"trusted":true},"outputs":[],"source":["class CNN:\n","    def __init__(self, \n","                 input_shape=(128, 128, 3), \n","                 unit_size_rate=0.05, \n","                 conv_padding='same', \n","                 conv_kernel_size=(3, 3),\n","                 conv_stride=1, \n","                 pool_padding='same', \n","                 pool_kernel_size=(2, 2), \n","                 pool_stride=2, \n","                 dropout=0, \n","                 l1_lambda=None,\n","                 l2_lambda=None, \n","                 pooling_type='max', \n","                 epochs=5, \n","                 batch_size=64, \n","                 verbose=1):\n","        self.input_shape = input_shape\n","        self.unit_size_rate = unit_size_rate\n","        self.conv_padding = conv_padding\n","        self.conv_kernel_size = conv_kernel_size\n","        self.conv_stride = conv_stride\n","        self.pool_padding = pool_padding\n","        self.pool_kernel_size = pool_kernel_size\n","        self.pool_stride = pool_stride\n","        self.dropout = dropout\n","        self.l1_lambda = l1_lambda\n","        self.l2_lambda = l2_lambda\n","        self.pooling_type = pooling_type\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.verbose = verbose\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        reg_l1 = None\n","        reg_l2 = None\n","\n","        if self.l1_lambda is not None:\n","            reg_l1 = tf.keras.regularizers.l1(self.l1_lambda)\n","\n","        if self.l2_lambda is not None:\n","            reg_l2 = tf.keras.regularizers.l2(self.l2_lambda)\n","\n","        if len(self.input_shape) == 2:\n","            # Add color channel for grayscale images\n","            self.input_shape = self.input_shape + (1,)\n","\n","        model = Sequential()\n","        model.add(Conv2D(\n","            int(32 * self.unit_size_rate),\n","            self.conv_kernel_size,\n","            strides=self.conv_stride,\n","            padding=self.conv_padding,\n","            activation='relu',\n","            kernel_regularizer=reg_l1,\n","            input_shape=self.input_shape))\n","\n","        if self.pooling_type == 'max':\n","            model.add(MaxPooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        elif self.pooling_type == 'avg':\n","            model.add(AveragePooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        else:\n","            raise ValueError(\"Invalid pooling_type. Use 'max' or 'avg'.\")\n","\n","        model.add(Conv2D(\n","            int(64 * self.unit_size_rate),\n","            self.conv_kernel_size,\n","            strides=self.conv_stride,\n","            padding=self.conv_padding,\n","            kernel_regularizer=reg_l2,\n","            activation='relu'))\n","\n","        if self.pooling_type == 'max':\n","            model.add(MaxPooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        elif self.pooling_type == 'avg':\n","            model.add(AveragePooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        else:\n","            raise ValueError(\"Invalid pooling_type. Use 'max' or 'avg'.\")\n","\n","        model.add(Conv2D(\n","            int(128 * self.unit_size_rate),\n","            self.conv_kernel_size,\n","            strides=self.conv_stride,\n","            padding=self.conv_padding,\n","            kernel_regularizer=reg_l2,\n","            activation='relu'))\n","\n","        if self.pooling_type == 'max':\n","            model.add(MaxPooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        elif self.pooling_type == 'avg':\n","            model.add(AveragePooling2D(\n","                pool_size=self.pool_kernel_size,\n","                strides=self.pool_stride,\n","                padding=self.pool_padding))\n","        else:\n","            raise ValueError(\"Invalid pooling_type. Use 'max' or 'avg'.\")\n","\n","        model.add(Flatten())\n","        model.add(Dense(\n","            units=int(128 * self.unit_size_rate),\n","            activation='relu',\n","            kernel_regularizer=reg_l2))\n","\n","        model.add(Dropout(self.dropout))\n","        model.add(Dense(units=1, activation='sigmoid'))\n","        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","        return model\n","\n","    def predict(self, X, threshold=0.5, verbose=0):\n","        return (self.model.predict(X, verbose=verbose) > threshold).astype(\"int32\")\n","\n","    def summary(self):\n","        self.model.summary()\n","\n","    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size, verbose=1):\n","        history = self.model.fit(\n","            X_train, y_train,\n","            batch_size=batch_size if batch_size else self.batch_size,\n","            epochs=epochs if epochs else self.epochs,\n","            verbose=verbose,\n","            validation_data=(X_val, y_val)\n","        )\n","        return history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:31:36.558372Z","iopub.status.busy":"2023-11-24T09:31:36.557374Z","iopub.status.idle":"2023-11-24T09:31:36.694876Z","shell.execute_reply":"2023-11-24T09:31:36.693734Z","shell.execute_reply.started":"2023-11-24T09:31:36.558333Z"},"trusted":true},"outputs":[],"source":["cnn = CNN(unit_size_rate=1,\n","          input_shape=(128,128,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:31:37.019366Z","iopub.status.busy":"2023-11-24T09:31:37.018616Z","iopub.status.idle":"2023-11-24T09:31:37.052147Z","shell.execute_reply":"2023-11-24T09:31:37.050975Z","shell.execute_reply.started":"2023-11-24T09:31:37.019327Z"},"trusted":true},"outputs":[],"source":["cnn.summary()"]},{"attachments":{"97dfa1ad-8632-40b6-9394-741979f21d58.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{},"source":["# 3. Splitting the Dataset:\n","![Spliting.png](attachment:97dfa1ad-8632-40b6-9394-741979f21d58.png)\n","\n","Split the dataset into training, validation, and test sets. The usual split ratios are \n","- 80% for training, \n","- 10% for validation, \n","- and 10% for testing. \n","Ensure that the classes are balanced in each split."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:04:11.948116Z","iopub.status.busy":"2023-11-24T09:04:11.947679Z","iopub.status.idle":"2023-11-24T09:04:12.420568Z","shell.execute_reply":"2023-11-24T09:04:12.4194Z","shell.execute_reply.started":"2023-11-24T09:04:11.94808Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:04:12.692152Z","iopub.status.busy":"2023-11-24T09:04:12.691724Z","iopub.status.idle":"2023-11-24T09:04:12.707438Z","shell.execute_reply":"2023-11-24T09:04:12.705921Z","shell.execute_reply.started":"2023-11-24T09:04:12.692119Z"},"trusted":true},"outputs":[],"source":["def viz_class_distribution(X, y, title, classe_labels, train_percent=0.6, val_percent=0.2, test_percent=0.2):\n","    '''\n","    :Parameters:\n","    - X (numpy array): The input feature matrix of shape (num_examples, num_features).\n","    - y (numpy array): The target labels of shape (num_examples,).\n","    - title (str): The title for the entire plot.\n","    - classes (list): A list of class labels, e.g., ['Normal', 'Tuberculosis'].\n","    - train_percent (float): Percentage of data for training set.\n","    - val_percent (float): Percentage of data for validation set.\n","    - test_percent (float): Percentage of data for test set.\n","\n","    :Returns:\n","    - X_train, y_train, X_test, y_test, X_val, y_val: The subsets of the data.\n","    '''\n","    assert train_percent + val_percent + test_percent == 1.0, \"Sum of train_percent, val_percent, and test_percent should be 1.0\"\n","\n","    # Convert y to integers\n","    y = y.astype(int)\n","\n","    # Split the data\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","        X, y, test_size=val_percent + test_percent, random_state=42, stratify=y)\n","\n","    test_size = test_percent / (val_percent + test_percent)\n","    X_val, X_test, y_val, y_test = train_test_split(\n","        X_temp, y_temp, test_size=test_size, random_state=42, stratify=y_temp)\n","\n","    # Create a subplot with 3 columns and 1 row\n","    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    # List of subset names\n","    subset_names = ['Train', 'Test', 'Validation']\n","    n = len(X)\n","\n","    # Iterate over subsets\n","    for i, subset in enumerate([(X_train, y_train), (X_test, y_test), (X_val, y_val)]):\n","        X_subset, y_subset = subset\n","        n_subset = len(y_subset)\n","        # Get the class counts\n","        class_counts = np.bincount(y_subset)\n","\n","        # Plot histogram for the current subset\n","        axs[i].bar(classe_labels, class_counts, color='#AA99FF')\n","        subtitle = r'$\\bf{' + subset_names[i] + \\\n","            '}$' + f' {int(n_subset/n*100)} %'\n","        axs[i].set_title(\n","            subtitle + f'\\n Size = {X_subset.shape[0]}', fontsize=18)\n","        axs[i].set_xlabel('Class')\n","        axs[i].set_ylabel('Number of examples')\n","\n","        # Add labels to the bars\n","        for j, count in enumerate(class_counts):\n","            axs[i].text(j, count, str(count), ha='center',\n","                        va='bottom', fontsize=12)\n","\n","    class_counts = np.bincount(y)\n","    class_balance_text = ' | '.join(\n","        [f'{class_label} ({count})' for class_label, count in zip(classe_labels, class_counts)])\n","    plt.suptitle(f'{title}' + f'\\n Training examples (X): {X.shape[0]}' +\n","                 f'\\nTarget (y): {class_balance_text}', fontsize=20)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return X_train, y_train, X_test, y_test, X_val, y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:04:13.751659Z","iopub.status.busy":"2023-11-24T09:04:13.751254Z","iopub.status.idle":"2023-11-24T09:04:16.024377Z","shell.execute_reply":"2023-11-24T09:04:16.023079Z","shell.execute_reply.started":"2023-11-24T09:04:13.751626Z"},"trusted":true},"outputs":[],"source":["X_train, y_train, X_test, y_test, X_val, y_val = viz_class_distribution(X=X, \n","                        y=y, \n","                        title='Class Distribution of Splitting', \n","                        classe_labels=['Normal', 'Tuberculosis'], \n","                        train_percent=0.8, \n","                        val_percent=0.1, \n","                        test_percent=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:04:19.257353Z","iopub.status.busy":"2023-11-24T09:04:19.256973Z","iopub.status.idle":"2023-11-24T09:04:36.442415Z","shell.execute_reply":"2023-11-24T09:04:36.441148Z","shell.execute_reply.started":"2023-11-24T09:04:19.257324Z"},"trusted":true},"outputs":[],"source":["images = [X_train, X_test, X_val]\n","subsets = ['X_{train}', 'X_{test}', 'X_{val}']\n","# For visualization\n","plot_gray_scale_histogram(images=images, titles=subsets, bins=555)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:30:03.904506Z","iopub.status.busy":"2023-11-24T10:30:03.903915Z","iopub.status.idle":"2023-11-24T10:30:04.158002Z","shell.execute_reply":"2023-11-24T10:30:04.156895Z","shell.execute_reply.started":"2023-11-24T10:30:03.904463Z"},"trusted":true},"outputs":[],"source":["model_config = {\n","    # (height, width, channels)\n","    'input_shape': (128, 128,3),\n","    'unit_size_rate': 0.05,\n","    'l1_lambda': None,\n","    'l2_lambda': None,\n","\n","    'conv_padding': 'same',\n","    'conv_kernel_size': (3, 3),\n","    'conv_stride': 1,\n","\n","    'pool_padding': 'same',\n","    'pool_kernel_size': (2, 2),\n","    'pool_stride': 2,\n","    \n","    'dropout': 0,\n","    'pooling_type': 'max'\n","}\n","\n","# Build model\n","model_1st = CNN(**model_config).build_model()\n","model_1st.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:31:31.64843Z","iopub.status.busy":"2023-11-24T10:31:31.647991Z","iopub.status.idle":"2023-11-24T10:31:31.654707Z","shell.execute_reply":"2023-11-24T10:31:31.65335Z","shell.execute_reply.started":"2023-11-24T10:31:31.648397Z"},"trusted":true},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:31:31.830096Z","iopub.status.busy":"2023-11-24T10:31:31.829646Z","iopub.status.idle":"2023-11-24T10:31:31.842064Z","shell.execute_reply":"2023-11-24T10:31:31.840808Z","shell.execute_reply.started":"2023-11-24T10:31:31.830054Z"},"trusted":true},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()\n","optimizer_config= optimizer.get_config()\n","optimizer_config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:31:32.030879Z","iopub.status.busy":"2023-11-24T10:31:32.030477Z","iopub.status.idle":"2023-11-24T10:37:58.26245Z","shell.execute_reply":"2023-11-24T10:37:58.26135Z","shell.execute_reply.started":"2023-11-24T10:31:32.03085Z"},"trusted":true},"outputs":[],"source":["start_time = time.time()\n","# verbose is progress bar; verbose = 1 display, verbose = don't display \n","verbose = 1\n","# Compile the model\n","model_1st.compile(loss='binary_crossentropy',\n","                   optimizer=optimizer,\n","                   metrics='accuracy')\n","\n","# Train the model\n","history = model_1st.fit(X_train, y_train, \n","                             epochs=30,\n","                             batch_size=64,\n","                             validation_data=(X_val, y_val),\n","                             verbose=verbose)\n","\n","end_time = time.time()\n","training_time = end_time - start_time\n","training_time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:37:58.26504Z","iopub.status.busy":"2023-11-24T10:37:58.264628Z","iopub.status.idle":"2023-11-24T10:37:58.269999Z","shell.execute_reply":"2023-11-24T10:37:58.268808Z","shell.execute_reply.started":"2023-11-24T10:37:58.265006Z"},"trusted":true},"outputs":[],"source":["print(f'Training time in minutes: {training_time/60:.2f} minutes')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T09:15:07.308448Z","iopub.status.busy":"2023-11-24T09:15:07.307982Z","iopub.status.idle":"2023-11-24T09:15:07.319843Z","shell.execute_reply":"2023-11-24T09:15:07.318376Z","shell.execute_reply.started":"2023-11-24T09:15:07.308416Z"},"trusted":true},"outputs":[],"source":["get_config = {\n","            'Optimizer parameters Configuration': {\n","                'optimizer_name': optimizer_config['name'],\n","                'learning_rate': optimizer_config['learning_rate'],\n","                'beta1': optimizer_config['beta_1'],\n","                'beta2': optimizer_config['beta_2'],\n","                'epsilon': optimizer_config['epsilon'],\n","            },\n","            'Hyperparameters Configuration': {\n","                'conv_padding': model_config['conv_padding'],\n","                'conv_kernel_size': model_config['conv_kernel_size'],\n","                'conv_stride': model_config['conv_stride'],\n","                'pool_padding': model_config['pool_padding'],\n","                'pool_kernel_size': model_config['pool_kernel_size'],\n","                'pool_stride': model_config['pool_stride'],\n","                'pooling_type': model_config['pooling_type'],\n","            },\n","            'Model Architecture Configuration': {\n","                'input_shape': model_config['input_shape'],\n","                'unit_size_rate': model_config['unit_size_rate']\n","            },\n","            'Model Training Configuration': {\n","                'batch_size': 20,\n","                'epoch': 64\n","            },\n","        }\n","get_config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:38:04.899319Z","iopub.status.busy":"2023-11-24T10:38:04.898814Z","iopub.status.idle":"2023-11-24T10:38:04.925262Z","shell.execute_reply":"2023-11-24T10:38:04.924045Z","shell.execute_reply.started":"2023-11-24T10:38:04.899281Z"}},"outputs":[],"source":["def get_history(history,\n","                model,\n","                training_time:float, \n","                ref_name:str, \n","                title:str):\n","\n","        best_epoch = np.argmax(history.history['val_accuracy'])\n","        best_train_epoch = np.argmax(history.history['accuracy'])\n","\n","        # Create a 1x2 grid of subplots\n","        fig, axes = plt.subplots(1, 2, figsize=(24, 15))\n","\n","        # Plot training and validation accuracy\n","        axes[0].plot(history.epoch, history.history['accuracy'],\n","                     label='Train Accuracy')\n","        axes[0].plot(history.epoch, history.history['val_accuracy'],\n","                     label='Validation Accuracy')\n","        axes[0].scatter(best_epoch, history.history['val_accuracy']\n","                        [best_epoch], color='r', label='Best Epoch (Validation)')\n","        axes[0].scatter(best_train_epoch, history.history['accuracy']\n","                        [best_train_epoch], color='g', label='Best Epoch (Train)')\n","\n","        # Annotate the best validation accuracy point\n","        best_accuracy = history.history['val_accuracy'][best_epoch]\n","        random_offset_x = 10\n","        random_offset_y = np.random.randint(-30, 30)\n","        axes[0].annotate(f'{best_accuracy:.3f}',\n","                         xy=(best_epoch, best_accuracy),\n","                         xytext=(random_offset_x, random_offset_y),\n","                         textcoords='offset points',\n","                         arrowprops=dict(arrowstyle='->'))\n","\n","        axes[0].set_xlabel('Epoch')\n","        axes[0].set_ylabel('Accuracy (%)')\n","        axes[0].set_title('Training and Validation Accuracy')\n","        axes[0].legend()\n","\n","        # Annotate the best training accuracy point\n","        best_train_accuracy = history.history['accuracy'][best_train_epoch]\n","        random_offset_y = np.random.randint(-30, 30)\n","        axes[0].annotate(f'{best_train_accuracy:.3f}',\n","                         xy=(best_train_epoch, best_train_accuracy),\n","                         xytext=(random_offset_x, random_offset_y),\n","                         textcoords='offset points',\n","                         arrowprops=dict(arrowstyle='->'))\n","\n","        # Plot training and validation loss\n","        axes[1].plot(history.epoch,\n","                    history.history['loss'], label='Train Loss')\n","        axes[1].plot(history.epoch, history.history['val_loss'],\n","                     label='Validation Loss')\n","        axes[1].scatter(best_epoch, history.history['val_loss']\n","                        [best_epoch], color='r', label='Best Epoch (Validation)')\n","        axes[1].scatter(best_train_epoch,history.history['loss']\n","                        [best_train_epoch], color='g', label='Best Epoch (Train)')\n","        axes[1].set_xlabel('Epoch')\n","        axes[1].set_ylabel('Loss')\n","        axes[1].set_title('Training and Validation Loss')\n","        axes[1].legend()\n","\n","        # Annotate the best validation loss point\n","        best_loss = history.history['val_loss'][best_epoch]\n","        random_offset_y = np.random.randint(-30, 30)\n","        axes[1].annotate(f'{best_loss:.3f}',\n","                         xy=(best_epoch, best_loss),\n","                         xytext=(random_offset_x, random_offset_y),\n","                         textcoords='offset points',\n","                         arrowprops=dict(arrowstyle='->'))\n","\n","        # Annotate the best training loss point\n","        best_train_loss = history.history['loss'][best_train_epoch]\n","        random_offset_y = np.random.randint(-30, 30)\n","        axes[1].annotate(f'{best_train_loss:.3f}',\n","                         xy=(best_train_epoch, best_train_loss),\n","                         xytext=(random_offset_x, random_offset_y),\n","                         textcoords='offset points',\n","                         arrowprops=dict(arrowstyle='->'))\n","\n","        # Calculate best validation accuracy in percentage\n","        best_accuracy_percentage = round(best_accuracy * 100, 2)\n","\n","        # String for performance, accuracy, and training time\n","        minutes = int(training_time // 60)\n","        seconds = int(training_time % 60)\n","\n","        # model summary\n","        stringlist = []\n","        model.summary(print_fn=lambda x: stringlist.append(x))\n","        short_model_summary = \"\\n\".join(stringlist)\n","\n","        # Set the text for the parameters on the right side\n","        performance = r'$\\bf{' + f'Ref. name: {ref_name}' + '}$' + f\"\\nBest Validation Accuracy: {best_accuracy_percentage}%\" + \\\n","            f\"\\nBest Train Accuracy: {history.history['accuracy'][best_train_epoch] * 100:.2f}%\" + \\\n","            f\", Training Time: {minutes} minutes {seconds} seconds\"\n","        fig.suptitle(r'$\\bf{' + title + '}$' +\n","                     '\\n' + performance, fontsize=18)\n","\n","        fig.text(1.05, 1.00, r'$\\bf{' + 'Config Parameters:' + '}$', fontsize=16,\n","                 color='black', ha='left', transform=plt.gcf().transFigure)\n","\n","        y_coord = 0.98  # Initial y-coordinate\n","\n","        # Set the line height between each parameter group and each parameter within a group\n","        line_height = 0.028\n","\n","        for group, params in get_config.items():\n","            fig.text(1.05, y_coord, r'$\\bf{' + f'{group}:' + '}$', fontsize=14,\n","                     color='black', ha='left', transform=plt.gcf().transFigure)\n","            y_coord -= line_height\n","\n","            for key, value in params.items():\n","                fig.text(1.05, y_coord, f\"{key}: {value}\", fontsize=12,\n","                         color='black', ha='left', transform=plt.gcf().transFigure)\n","                y_coord -= line_height\n","\n","        fig.text(1.05, 0.01, r'$\\bf{' + f'Model Summary:' + '}$' + f'\\n{short_model_summary}',\n","                 fontsize=10, color='black', ha='left', transform=plt.gcf().transFigure)\n","\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:38:05.179702Z","iopub.status.busy":"2023-11-24T10:38:05.178503Z","iopub.status.idle":"2023-11-24T10:38:06.431959Z","shell.execute_reply":"2023-11-24T10:38:06.430243Z","shell.execute_reply.started":"2023-11-24T10:38:05.179661Z"},"trusted":true},"outputs":[],"source":["title = 'Experiment 1: Build 1st CNNs model'\n","get_history(history=history,\n","            model=model_1st,\n","            training_time=training_time, \n","            title=title, \n","            ref_name='Original Dataset')"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:29:59.404126Z","iopub.status.busy":"2023-11-24T11:29:59.403648Z","iopub.status.idle":"2023-11-24T11:29:59.409972Z","shell.execute_reply":"2023-11-24T11:29:59.409021Z","shell.execute_reply.started":"2023-11-24T11:29:59.404093Z"},"trusted":true},"outputs":[],"source":["import json\n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score, roc_curve, roc_auc_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:34:34.27216Z","iopub.status.busy":"2023-11-24T11:34:34.271701Z","iopub.status.idle":"2023-11-24T11:34:34.28513Z","shell.execute_reply":"2023-11-24T11:34:34.284027Z","shell.execute_reply.started":"2023-11-24T11:34:34.272123Z"},"trusted":true},"outputs":[],"source":[" def evaluate(estimator_name, estimator, X, y_actual, subset_name, training_time, threshold=None):\n","\n","        # Model Prediction/Y hat\n","        y_pred = estimator.predict(\n","            X) \n","        # Handling Output Layer of Activation='sigmoid'\n","        threshold = threshold if threshold else 0.5\n","        y_pred_binary = (y_pred > threshold).astype(int).flatten()\n","\n","        # Calculate ROC curve and AUC score\n","        fpr, tpr, thresholds = roc_curve(y_actual, y_pred_binary)\n","        auc_score = roc_auc_score(y_actual, y_pred_binary)\n","        # Classification Report\n","        report = classification_report(\n","            y_actual, y_pred_binary, output_dict=True)\n","        report_dict = json.loads(json.dumps(report))\n","\n","        # Extract precision and recall from report_dict\n","        precision = report_dict['weighted avg']['precision']\n","        recall = report_dict['weighted avg']['recall']\n","        accuracy = report_dict['accuracy']\n","        f1 = report_dict['weighted avg']['f1-score']\n","\n","        # Confusion Matrix\n","        cm = confusion_matrix(y_actual, y_pred_binary)\n","        TP = cm[0][0]\n","        FP = cm[0][1]\n","        FN = cm[1][0]\n","        TN = cm[1][1]\n","        total_seconds = training_time\n","        minutes = int(training_time // 60)\n","        seconds = int(training_time % 60)\n","        training_time_ = \"{} minutes {} seconds\".format(minutes, seconds)\n","\n","        accuracy_train = history.history['accuracy']\n","        accuracy_val = history.history['val_accuracy']\n","\n","        performance_info = {\n","            'Model': estimator_name,\n","            'accuracy_train': accuracy_train,\n","            'accuracy_val': accuracy_val,\n","            'Subset': subset_name,\n","            'Training time': training_time_,\n","            'Training in seconds': total_seconds,\n","            'TP': TP,\n","            'FP': FP,\n","            'FN': FN,\n","            'TN': TN,\n","            'Precision': precision,\n","            'Recall': recall,\n","            'F1': f1,\n","            'AUC': auc_score,\n","            'Accuracy': accuracy,\n","        }\n","        return pd.DataFrame([performance_info])"]},{"cell_type":"markdown","metadata":{},"source":["## 5.1 Training set Performance "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:34:34.831701Z","iopub.status.busy":"2023-11-24T11:34:34.830728Z","iopub.status.idle":"2023-11-24T11:34:44.141373Z","shell.execute_reply":"2023-11-24T11:34:44.140279Z","shell.execute_reply.started":"2023-11-24T11:34:34.831662Z"},"trusted":true},"outputs":[],"source":["evaluate(estimator_name='CNN', \n","         estimator=model_1st, \n","         X=X_train, \n","         y_actual=y_train, \n","         subset_name='Training', \n","         training_time=training_time\n","         )"]},{"cell_type":"markdown","metadata":{},"source":["## 5.2 Test set Performance "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:34:49.402679Z","iopub.status.busy":"2023-11-24T11:34:49.402275Z","iopub.status.idle":"2023-11-24T11:34:51.092332Z","shell.execute_reply":"2023-11-24T11:34:51.091185Z","shell.execute_reply.started":"2023-11-24T11:34:49.402648Z"},"trusted":true},"outputs":[],"source":["evaluate(estimator_name='CNN', \n","         estimator=model_1st, \n","         X=X_test, \n","         y_actual=y_test, \n","         subset_name='Test', \n","         training_time=training_time\n","         )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:36:53.504733Z","iopub.status.busy":"2023-11-24T11:36:53.504056Z","iopub.status.idle":"2023-11-24T11:36:53.515221Z","shell.execute_reply":"2023-11-24T11:36:53.514272Z","shell.execute_reply.started":"2023-11-24T11:36:53.504686Z"},"trusted":true},"outputs":[],"source":["def plot_confusion_matrix(y_actual, y_pred, subset_name):\n","    y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n","    cm = confusion_matrix(y_actual, y_pred_binary)\n","    labels = ['Negative', 'Positive']\n","\n","    # Extract TP, TN, FP, FN\n","    tn, fp, fn, tp = cm.ravel()\n","    sensitivity = tp / (tp + fn)  # Sensitivity or True Positive Rate\n","    specificity = tn / (tn + fp)  # Specificity or True Negative Rate\n","\n","    plt.figure(figsize=(18, 9))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=labels, yticklabels=labels)\n","\n","\n","\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.title(f'Confusion Matrix - {subset_name}\\n'\n","              f'True Positives: {tp} | True Negatives: {tn}\\n'\n","              f'False Positives: {fp} (Type I error) | False Negatives: {fn} (Type II error)\\n'\n","              f'Sensitivity (True Positive Rate): {sensitivity:.4f} | Specificity (True Negative Rate): {specificity:.4f}',\n","              loc='center', wrap=True)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:36:54.039136Z","iopub.status.busy":"2023-11-24T11:36:54.038698Z","iopub.status.idle":"2023-11-24T11:36:55.598833Z","shell.execute_reply":"2023-11-24T11:36:55.597558Z","shell.execute_reply.started":"2023-11-24T11:36:54.039101Z"},"trusted":true},"outputs":[],"source":["y_pred_test = model_1st.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["## 5.3 Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:36:55.601483Z","iopub.status.busy":"2023-11-24T11:36:55.600831Z","iopub.status.idle":"2023-11-24T11:36:56.255223Z","shell.execute_reply":"2023-11-24T11:36:56.253984Z","shell.execute_reply.started":"2023-11-24T11:36:55.601444Z"},"trusted":true},"outputs":[],"source":["plot_confusion_matrix(y_actual=y_test, \n","                      y_pred=y_pred_test, \n","                      subset_name='Test')"]},{"cell_type":"markdown","metadata":{},"source":["## 5.4 ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:37:21.906761Z","iopub.status.busy":"2023-11-24T11:37:21.906259Z","iopub.status.idle":"2023-11-24T11:37:21.91332Z","shell.execute_reply":"2023-11-24T11:37:21.912296Z","shell.execute_reply.started":"2023-11-24T11:37:21.90667Z"},"trusted":true},"outputs":[],"source":["def plot_roc(y_actual, y_pred, subset_name):\n","    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n","    # Print ROC Curve\n","    plt.figure(figsize=(18, 9))\n","    plt.plot(fpr, tpr, marker='o', linestyle='-',)\n","    plt.xlabel('False positive rate (1-Specificity)')\n","    plt.ylabel('True positive rate (Sensitivity)')\n","    plt.title(f'ROC Curve - {subset_name}')\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T11:37:31.394053Z","iopub.status.busy":"2023-11-24T11:37:31.393465Z","iopub.status.idle":"2023-11-24T11:37:33.089969Z","shell.execute_reply":"2023-11-24T11:37:33.088781Z","shell.execute_reply.started":"2023-11-24T11:37:31.394Z"},"trusted":true},"outputs":[],"source":["plot_roc(y_actual=y_test, y_pred=y_pred_test, subset_name='Test')"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Different Image Input Sizes\n","## **Expected Outcome:**\n","\n","We expect that resizing the images to 128x128 might lead to a slight decrease in accuracy compared to using the original 512x512 images. However, the reduced image size may offer faster training and inference times, making it a more practical choice for resource-constrained environments. The experimental results will provide insights into the trade-offs between model accuracy and computational efficiency when dealing with different image sizes in TB detection using CNNs."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:14:11.386971Z","iopub.status.busy":"2023-11-24T10:14:11.386487Z","iopub.status.idle":"2023-11-24T10:17:08.564069Z","shell.execute_reply":"2023-11-24T10:17:08.562652Z","shell.execute_reply.started":"2023-11-24T10:14:11.386923Z"},"trusted":true},"outputs":[],"source":["target_shapes = [(16, 16),(32, 32), (64, 64), (128, 128)]\n","for target_shape in target_shapes:\n","    X_resized = tf.image.resize(X, target_shape).numpy()\n","    model_config = {\n","        # (height, width, channels)\n","        'input_shape': target_shape +(3,),\n","        'unit_size_rate': 0.05,\n","        'l1_lambda': None,\n","        'l2_lambda': None,\n","\n","        'conv_padding': 'same',\n","        'conv_kernel_size': (3, 3),\n","        'conv_stride': 1,\n","\n","        'pool_padding': 'same',\n","        'pool_kernel_size': (2, 2),\n","        'pool_stride': 2,\n","\n","        'dropout': 0,\n","        'pooling_type': 'max'\n","    }\n","\n","    # Build model\n","    model = CNN(**model_config)\n","\n","    X_train, X_temp, y_train, y_temp = train_test_split(\n","    X_resized, y, test_size=0.2, random_state=42, stratify=y)\n","\n","    X_test, X_val, y_test, y_val = train_test_split(\n","        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","    # Train the model\n","    start_time = time.time()\n","\n","    history = model.train(X_train, y_train, X_val, y_val,\n","                                 epochs=5,\n","                                 batch_size=64,\n","                                 verbose=0)\n","    end_time = time.time()\n","    training_time = end_time - start_time\n","    mean_acc = np.mean(history.history['val_accuracy'])\n","    show_image(images=X_resized, target=y,\n","               title=f'Resize the input images {target_shape}\\nAvg. accuracy = {mean_acc:2f}\\nTraining time: {training_time/60:.2f} minutes', \n","               num_display=4, \n","               num_cols=4, \n","               random_mode=False)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Model Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:17:27.59863Z","iopub.status.busy":"2023-11-24T10:17:27.598231Z","iopub.status.idle":"2023-11-24T10:17:27.611681Z","shell.execute_reply":"2023-11-24T10:17:27.610237Z","shell.execute_reply.started":"2023-11-24T10:17:27.5986Z"},"trusted":true},"outputs":[],"source":["def compare_actual_and_predicted_xray(estimator, images, target, title, num_display=16, num_cols=4, random_mode=False):\n","    '''\n","    Compare Actual X-ray Images with Model Predictions.\n","\n","    Parameters:\n","        estimator: Model used for predictions.\n","        images (ndarray): Input data as a numpy array.\n","        target (ndarray): Target data as a numpy array.\n","        title (str): Title of the plot.\n","        num_display (int, optional): Number of images to display. Default is 16.\n","        num_cols (int, optional): Number of columns in the plot. Default is 4.\n","        random_mode (bool, optional): If True, display images randomly. If False, display the first num_display images. Default is False.\n","\n","    Returns:\n","        None\n","\n","    This function generates a visual comparison between actual X-ray images and their corresponding model predictions. It displays a grid of images with labels to show whether the model's predictions match the actual target values. The grid is organized in rows and columns based on the specified parameters.\n","\n","    The function does not return any values; it displays the comparison plot directly.\n","    '''\n","\n","    n_cols = min(num_cols, num_display)\n","    n_rows = int(np.ceil(num_display / n_cols))\n","\n","    title = r'$\\bf{' + \"Actual-Image\" + '}$' + \" vs \" + \\\n","        r'$\\bf{' + \"Model-Prediction\" + '}$'\n","    y_hat = estimator.predict(images)\n","    prediction = (y_hat > 0.5).astype(int).flatten()\n","\n","    n_images = min(num_display, len(images))\n","    if random_mode:\n","        random_indices = np.random.choice(\n","            len(images), num_display, replace=False)\n","    else:\n","        random_indices = np.arange(num_display)\n","\n","    fig, axes = plt.subplots(\n","        nrows=n_rows, ncols=n_cols, figsize=(20, 4*n_rows))\n","    for i, ax in enumerate(axes.flatten()):\n","        if i >= n_images:  # Check if the index exceeds the available number of images\n","            break\n","        # Incase (Did PCA)\n","        index = random_indices[i]\n","        if len(images.shape) == 2:\n","            image = images[index].reshape((128, 128)).astype(int)\n","        else:\n","            image = images[index]\n","        actual_label = target[index]\n","        model_pred_label = prediction[index]\n","        model_prob = '{:.3f}'.format(float(y_hat[index]))\n","        ax.imshow(\n","            image, cmap='gray' if actual_label == model_pred_label else 'OrRd')\n","        ax.set_title(\n","            f\"Actual: {actual_label},\\nModel Prediction: {model_pred_label}\\nProbability: {model_prob}\")\n","    plt.suptitle(f\"{title} (Displaying {num_display} Images)\",\n","                 fontsize=16, fontweight='bold')\n","\n","    fig.set_facecolor('white')\n","    plt.tight_layout()  \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:17:27.823355Z","iopub.status.busy":"2023-11-24T10:17:27.822412Z","iopub.status.idle":"2023-11-24T10:17:46.64834Z","shell.execute_reply":"2023-11-24T10:17:46.646938Z","shell.execute_reply.started":"2023-11-24T10:17:27.823316Z"},"trusted":true},"outputs":[],"source":["compare_actual_and_predicted_xray(\n","                estimator=model_1st,\n","                images=X_test,\n","                target=y_test, \n","                title=None,\n","                num_display=100,\n","                num_cols=10, \n","                random_mode=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Lung segmentation\n","**Objective**: adapt this Lung segmentation for LIME\n","steps:\n","- Equalize the image with CLAHE\n","- Create a threshold mask to separate tissue by pixel intensity\n","- Find regions in the threshold\n","- Remove borders\n","- Fill small holes\n","- Extract lung areas\n","\n","**Reference**: Lung Segmentation without CNN->https://www.kaggle.com/code/davidbroberts/lung-segmentation-without-cnn/notebook\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:20:07.811719Z","iopub.status.busy":"2023-11-24T10:20:07.811014Z","iopub.status.idle":"2023-11-24T10:20:09.579396Z","shell.execute_reply":"2023-11-24T10:20:09.578151Z","shell.execute_reply.started":"2023-11-24T10:20:07.811672Z"},"trusted":true},"outputs":[],"source":["from skimage.filters import threshold_otsu\n","from skimage.color import label2rgb\n","from skimage.morphology import closing, square\n","from skimage.measure import label, regionprops\n","from skimage.segmentation import clear_border"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:21:35.220759Z","iopub.status.busy":"2023-11-24T10:21:35.220336Z","iopub.status.idle":"2023-11-24T10:21:35.236293Z","shell.execute_reply":"2023-11-24T10:21:35.235137Z","shell.execute_reply.started":"2023-11-24T10:21:35.220726Z"},"trusted":true},"outputs":[],"source":["class ImageSegmentation:\n","    def __init__(self, input_shape):\n","        self.input_shape = input_shape\n","\n","    def lung_segmentation(self, X):\n","        if X.shape != self.input_shape:\n","            X = X[:, :, 0]\n","\n","        # Equalize the image with CLAHE\n","        X = exposure.equalize_adapthist(\n","            X, kernel_size=None, clip_limit=0.01, nbins=256)\n","\n","        # Create a binary threshold mask and apply it to the image\n","        thresh = threshold_otsu(image=X, nbins=256, hist=None)\n","        thresh = X > thresh\n","        bw = closing(X > thresh, square(1))\n","\n","        # Clean up the borders\n","        cleared = clear_border(bw)\n","\n","        # Label image regions\n","        label_image = label(cleared)\n","        image_label_overlay = label2rgb(\n","            label_image,\n","            image=X,\n","            bg_label=0,\n","            bg_color=(0, 0, 0))\n","\n","        return image_label_overlay, label_image\n","\n","    def show_segmented_images(self,images, target, title, num_display=16, num_cols=4, cmap='gray', random_mode=False,only_segmented=True):\n","        '''\n","        :Parameters\n","            images (ndarray (n,)): Input data as a numpy array.\n","            target (ndarray (n,)): Target data as a numpy array.\n","            title (String): Title of the plot.\n","            num_display (int): Number of images to display. Default is 16.\n","            num_cols (int): Number of columns in the plot. Default is 4.\n","            cmap (str): Color map for displaying images. Default is 'gray'.\n","            random_mode (bool): If True, display images randomly. If False, display the first num_display images. Default is False.\n","        '''\n","        # Determine the number of rows based on the num_cols parameter\n","        n_cols = min(num_cols, num_display)\n","        n_rows = int(np.ceil(num_display / n_cols))\n","\n","        n_images = min(num_display, len(images))\n","        if random_mode:\n","            random_indices = np.random.choice(\n","                len(images), num_display, replace=False)\n","        else:\n","            random_indices = np.arange(num_display)\n","\n","        fig, axes = plt.subplots(\n","            nrows=n_rows, ncols=n_cols, figsize=(20, 4*n_rows))\n","        for i, ax in enumerate(axes.flatten()):\n","            if i >= n_images:  # Check if the index exceeds the available number of images\n","                break\n","            # Incase (Did PCA)\n","            index = random_indices[i]\n","            if len(images.shape) == 2:\n","                image = images[index].reshape((128, 128)).astype(int)\n","            else:\n","                image = images[index]\n","\n","            image_label_overlay, label_image = self.lung_segmentation(\n","                image)\n","                \n","            ax.imshow(\n","                label_image if only_segmented else image_label_overlay, cmap=cmap)\n","            ax.set_title(\"Target: {}\".format(target[index]))\n","\n","            # Add image index as text\n","            ax.text(0.5, -0.15, f'Image Index: {index}', transform=ax.transAxes,\n","                    fontsize=10, ha='center')\n","\n","        plt.suptitle(f\"{title} (Displaying {num_display} Images)\",\n","                    fontsize=16, fontweight='bold')\n","\n","        fig.set_facecolor('white')\n","        plt.tight_layout()  # Added to ensure proper spacing between subplots\n","        return plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:21:42.006825Z","iopub.status.busy":"2023-11-24T10:21:42.006397Z","iopub.status.idle":"2023-11-24T10:21:42.012257Z","shell.execute_reply":"2023-11-24T10:21:42.010767Z","shell.execute_reply.started":"2023-11-24T10:21:42.006791Z"},"trusted":true},"outputs":[],"source":["img_segmetator=ImageSegmentation(input_shape=(128,128))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:21:59.110087Z","iopub.status.busy":"2023-11-24T10:21:59.109608Z","iopub.status.idle":"2023-11-24T10:22:02.819509Z","shell.execute_reply":"2023-11-24T10:22:02.818121Z","shell.execute_reply.started":"2023-11-24T10:21:59.110051Z"},"trusted":true},"outputs":[],"source":["img_segmetator.show_segmented_images(images=X,\n","                                     target=y,\n","                                     num_display=16, num_cols=8,\n","                                     title='Lung Segmentation',\n","                                     cmap='binary',\n","                                     random_mode=False,\n","                                     only_segmented=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:22:14.584701Z","iopub.status.busy":"2023-11-24T10:22:14.584235Z","iopub.status.idle":"2023-11-24T10:22:18.041092Z","shell.execute_reply":"2023-11-24T10:22:18.039909Z","shell.execute_reply.started":"2023-11-24T10:22:14.584664Z"},"trusted":true},"outputs":[],"source":["img_segmetator.show_segmented_images(images=X,\n","                                     target=y,\n","                                     num_display=16, num_cols=8,\n","                                     title='Lung Segmentation',\n","                                     cmap='binary',\n","                                     random_mode=False,\n","                                     only_segmented=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 9. Explainability\n","## 9.1 Lime Library\n","\n","**Local Interpretable Model-Agnostic Explanations (LIME)** \n","\n","is a library that explains predictions of machine learning models in an interpretable and understandable way. LIME works by training a local surrogate model around each prediction instance to approximate the behavior of the complex model. The surrogate model is then used to explain the prediction's outcome by highlighting the features that contributed the most to the decision.\n","\n","**LIME**\n","\n","is short for \"Local Interpretable **Model-agnostic Explanations.**\" It deals with explaining predictions made by **image-based** models, highlighting which areas of the input image were most influential in the model's decision-making process.\n","\n","**Use to**\n","\n","- **Interpretable** Literally translated, LIME is a method for translating the behavior of models that are difficult to understand so that normal people like us can understand them.\n","\n","- **Model-Agnostic** This means that the LIME method can be used with any model, whether the model is a model that can read its behavior (such as linear regression, decision trees) or the model is so complex that we don't know its inner behavior (such as a neural network. ), which means we can view the model we use as a black-box.\n","\n","- used to **explain why** a specific image was classified as it was by identifying the segmented areas (features) that had the most impact on the model's prediction.\n","- To **visualize** and **understand** Model's predictions\n","\n","**Read more:** \n","- Community: https://notebook.community/marcotcr/lime/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras\n","- Document: https://lime-ml.readthedocs.io/en/latest/lime.html\n","- How to Use LIME to Interpret Predictions of ML Models?: https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:51:51.907403Z","iopub.status.busy":"2023-11-24T10:51:51.906807Z","iopub.status.idle":"2023-11-24T10:51:51.948008Z","shell.execute_reply":"2023-11-24T10:51:51.946479Z","shell.execute_reply.started":"2023-11-24T10:51:51.907363Z"},"trusted":true},"outputs":[],"source":["# Lime\n","import lime\n","from lime.lime_image import LimeImageExplainer, ImageExplanation\n","# Ski\n","from skimage.segmentation import mark_boundaries\n","from skimage.color import gray2rgb\n","from skimage.transform import resize\n","\n","import skimage.segmentation\n","\n","# common\n","import numpy as np\n","import tensorflow as tf\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import random\n","\n","import cv2\n","import matplotlib.patches as mpatches\n","from skimage.filters import threshold_otsu\n","from skimage.exposure import equalize_adapthist\n","from skimage.color import label2rgb\n","from skimage.segmentation import clear_border\n","from skimage.measure import label, regionprops\n","from skimage.morphology import square, closing\n","from skimage.segmentation import felzenszwalb, slic, quickshift, watershed\n","from skimage.filters import sobel\n","from skimage.color import rgb2gray\n","\n","class Explainer():\n","    def __init__(self, estimator, input_shape=None, threshold=None):\n","        self.estimator = estimator\n","        self.input_shape = input_shape\n","\n","    def lime(self, X, y, top_labels, num_samples, num_features, class_names, num_display, method='slic', random_mode=False, cmap='gray'):\n","        '''\n","        Explain and visualize model predictions using LIME (Local Interpretable Model-Agnostic Explanations).\n","\n","        Parameters:\n","            X (ndarray): Input data as a numpy array.\n","            y (ndarray): Target data as a numpy array.\n","            top_labels (int): Top coefficients to show (e.g., 1 for binary classification).\n","            num_samples (int): Number of perturbations to generate.\n","            num_features (int): Number of highlighted features (segmented areas) to extract.\n","            model (object): Trained machine learning model.\n","            class_names (list): List of class names.\n","            num_display (int): Number of images to display.\n","            random_mode (bool, optional): If True, display images randomly. Default is False.\n","            cmap (str, optional): Colormap for displaying images. Default is 'gray'.\n","\n","        Returns:\n","            None\n","\n","        This function explains and visualizes model predictions using LIME. It generates explanations for model predictions and displays them along with various image representations.\n","\n","        Note:\n","        - The `X` input should have a shape like (num_images, width, height, channels) (e.g., (100, 128, 128, 1)).\n","        - The `class_names` list should contain class labels.\n","        - The function displays the explanations and does not return any values.\n","        '''\n","\n","        # Model prediction\n","        \n","        y_hat = self.estimator.predict(X)\n","        prediction = (y_hat > 0.5).astype(int).flatten()\n","\n","        n_cols = 5  # Display 5 images in each row\n","\n","        title = r'$\\bf{' + \"Actual-Image\" + '}$' + \" vs \" + \\\n","            r'$\\bf{' + \"Model-Prediction\" + '}$'\n","\n","        n_images = min(num_display, len(X))\n","\n","        if random_mode:\n","            random_indices = np.random.choice(\n","                len(X), num_display, replace=False)\n","        else:\n","            random_indices = np.arange(num_display)\n","\n","        for i, idx in enumerate(random_indices):\n","            \n","            X_resized = tf.image.resize(X[idx], self.input_shape).numpy()\n","#             # Change to RBG for input in Lime\n","#             rgb_resized_image = cv2.cvtColor(X_resized, cv2.COLOR_GRAY2RGB)\n","            rgb_resized_image = X_resized\n","            explainer = LimeImageExplainer()\n","\n","            def model_pred_fn(images):\n","                y_hat = self.estimator.predict(images)\n","                return y_hat\n","            \n","            def segment_fn(image):\n","                if method == 'felzenszwalb':\n","                    segments = felzenszwalb(image, scale=200, sigma=0.5, min_size=50)\n","                elif method == 'slic':\n","                    segments = slic(image, n_segments=50, compactness=10, sigma=1)\n","                elif method == 'quickshift':\n","                    segments = quickshift(image, kernel_size=2, max_dist=50, ratio=0.5)\n","                elif method == 'watershed':\n","                    gradient = sobel(rgb2gray(image))\n","                    segments = watershed(gradient, markers=50, compactness=0.001)\n","                elif method == 'lung_segmentation':\n","                    segments = self.lung_segmentation(image)\n","                else:\n","                    raise ValueError(\n","                        \"Invalid segmentation method. Supported methods: 'felzenszwalb', 'slic', 'quickshift', 'watershed', 'lung_segmentation'\")\n","\n","                return segments\n","\n","            explanation = explainer.explain_instance(\n","                rgb_resized_image,\n","                model_pred_fn,\n","                top_labels=top_labels,\n","                hide_color=0,\n","                num_samples=num_samples,\n","                segmentation_fn=segment_fn,\n","            )\n","            #  positive_only=True if prediction[idx] ==0  else False,\n","            temp, mask = explanation.get_image_and_mask(\n","                explanation.top_labels[0],\n","                positive_only=False,\n","                num_features=num_features,\n","                hide_rest=False,\n","                min_weight=0.0\n","                )\n","\n","            # Create a figure with 1 row and 5 columns\n","            fig, axes = plt.subplots(1, n_cols, figsize=(30, 6))\n","\n","            # Add a big header title above the subplots\n","            plt.suptitle(\n","                title, fontsize=16)\n","\n","            # First axis: Actual Image\n","            axes[0].imshow(X_resized, cmap=cmap) \n","            actual_title = r'$\\bf{' + 'Actual' + '}$'\n","            actual_cls = class_names[int(y[idx])]\n","\n","            model_title = r'$\\bf{' + 'Model-Prediction' + '}$'\n","            model_cls = class_names[int(prediction[idx])]\n","            model_prob = '{:.9f}'.format(float(y_hat[idx]))\n","            axes[0].set_title(actual_title + '\\nTarget: ' + actual_cls)\n","\n","            image_label_overlay = label2rgb(\n","                segment_fn(rgb_resized_image),\n","                image=rgb_resized_image,\n","                bg_label=0,\n","                bg_color=(0, 0, 0))\n","\n","            axes[1].imshow(X_resized, cmap=cmap)\n","            axes[1].imshow(image_label_overlay)\n","\n","            axes[1].set_title(model_title + '\\nTarget: ' + model_cls)\n","\n","            axes[2].imshow(segment_fn(rgb_resized_image),cmap='binary')\n","            axes[2].set_title(\n","                r'$\\bf{' + 'Segmentation' + '}$' + f'\\nProbability: ' + model_prob)\n","            \n","            # Third axis: Explanation Image\n","            axes[3].imshow(mark_boundaries(\n","                rgb_resized_image, segment_fn(rgb_resized_image)))\n","            axes[3].set_title(\n","                r'$\\bf{' + 'LIME-Explanation' + '}$' + f'\\nnumber of segments: {len(np.unique(segment_fn(rgb_resized_image)))}')\n","            temp, mask = explanation.get_image_and_mask(\n","                0,\n","                positive_only=True if prediction[idx] == 0 else False,\n","                num_features=num_features,\n","                hide_rest=False,\n","                min_weight=0.0\n","            )\n","            # Third axis: Explanation Image\n","            axes[4].imshow(mark_boundaries(\n","                temp, mask))\n","            axes[4].set_title(\n","                r'$\\bf{' + 'LIME-Explanation' + '}$' + f'\\nPositive and Negative regions')\n","\n","            # Add the image index text\n","            plt.text(0.5, -0.15, f'Image Index: {idx}', transform=axes[0].transAxes,\n","                     fontsize=10, ha='center')\n","\n","            plt.tight_layout()\n","\n","            plt.show()\n","\n","    def diff_sementation(self, X):\n","        # Generate a random sample index\n","        sample_index = random.randint(0, X.shape[0] - 1)\n","\n","#         # Get the single grayscale sample\n","#         grayscale_sample = X[sample_index]\n","\n","#         # Convert to RGB by repeating the single channel along the third axis\n","#         img = np.repeat(grayscale_sample, 3, axis=2)\n","\n","        segments_fz = felzenszwalb(img, scale=200, sigma=0.5, min_size=50)\n","        segments_slic = slic(img, n_segments=50, compactness=10, sigma=1)\n","        segments_quick = quickshift(img, kernel_size=2, max_dist=50, ratio=0.5)\n","        gradient = sobel(rgb2gray(img))\n","        segments_watershed = watershed(gradient, markers=50, compactness=0.001)\n","        segments_equalize_adapthist = self.lung_segmentation(img)\n","\n","        fig, ax = plt.subplots(1, 5, figsize=(30, 6))\n","\n","        ax[0].imshow(mark_boundaries(img, segments_fz))\n","        ax[0].set_title(\"Felzenszwalbs's method\")\n","        ax[0].text(\n","            0.5, -0.05, f\"number of segments: {len(np.unique(segments_fz))}\", transform=ax[0].transAxes, ha='center')\n","\n","        ax[1].imshow(mark_boundaries(img, segments_slic))\n","        ax[1].set_title('SLIC')\n","        ax[1].text(\n","            0.5, -0.05, f\"number of segments: {len(np.unique(segments_slic))}\", transform=ax[1].transAxes, ha='center')\n","\n","        ax[2].imshow(mark_boundaries(img, segments_quick))\n","        ax[2].set_title('Quickshift')\n","        ax[2].text(\n","            0.5, -0.05, f\"number of segments: {len(np.unique(segments_quick))}\", transform=ax[2].transAxes, ha='center')\n","\n","        ax[3].imshow(mark_boundaries(img, segments_watershed))\n","        ax[3].set_title('Compact watershed')\n","        ax[3].text(\n","            0.5, -0.05, f\"number of segments: {len(np.unique(segments_watershed))}\", transform=ax[3].transAxes, ha='center')\n","        ax[4].imshow(mark_boundaries(img, segments_equalize_adapthist))\n","        ax[4].set_title('lung_segmentation')\n","        ax[4].text(\n","            0.5, -0.05, f\"number of segments: {len(np.unique(segments_equalize_adapthist))}\", transform=ax[4].transAxes, ha='center')\n","\n","        for a in ax.ravel():\n","            a.set_axis_off()\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def lung_segmentation(self, X):\n","        # Single of exmaple\n","        X = X[:, :, 0]\n","\n","        # Even out the contrast with CLAHE\n","        X_equalized = equalize_adapthist(\n","            X, kernel_size=None, clip_limit=0.05, nbins=256)\n","\n","        # Make a binary threshold mask and apply it to the image\n","        thresh = threshold_otsu(image=X_equalized, nbins=256, hist=None)\n","        thresh = X_equalized > thresh\n","        bw = closing(X_equalized > thresh, square(1))\n","\n","        # Clean up the borders\n","        cleared = clear_border(bw)\n","\n","        # Label image regions\n","        label_image = label(cleared)\n","        \n","\n","        return label_image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:51:52.24689Z","iopub.status.busy":"2023-11-24T10:51:52.246466Z","iopub.status.idle":"2023-11-24T10:51:52.252464Z","shell.execute_reply":"2023-11-24T10:51:52.251124Z","shell.execute_reply.started":"2023-11-24T10:51:52.246857Z"},"trusted":true},"outputs":[],"source":["explnr = Explainer(estimator=model_1st, input_shape=(128, 128))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:51:52.395556Z","iopub.status.busy":"2023-11-24T10:51:52.39462Z","iopub.status.idle":"2023-11-24T10:51:53.377494Z","shell.execute_reply":"2023-11-24T10:51:53.376154Z","shell.execute_reply.started":"2023-11-24T10:51:52.395515Z"},"trusted":true},"outputs":[],"source":["explnr.diff_sementation(X=X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:51:53.381307Z","iopub.status.busy":"2023-11-24T10:51:53.37985Z","iopub.status.idle":"2023-11-24T10:51:53.419261Z","shell.execute_reply":"2023-11-24T10:51:53.418017Z","shell.execute_reply.started":"2023-11-24T10:51:53.381242Z"},"trusted":true},"outputs":[],"source":["model_1st.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:51:53.421678Z","iopub.status.busy":"2023-11-24T10:51:53.421237Z","iopub.status.idle":"2023-11-24T10:52:37.207213Z","shell.execute_reply":"2023-11-24T10:52:37.206018Z","shell.execute_reply.started":"2023-11-24T10:51:53.421646Z"},"trusted":true},"outputs":[],"source":["explnr.lime(X=X_test,\n","            y=y_test,\n","            top_labels=1,\n","            num_samples=1000,\n","            num_features=5,\n","            class_names=['Normal', 'Tuberculosis'],\n","            num_display=10,\n","            method='lung_segmentation',\n","            random_mode=True,\n","            cmap='gray'\n","            )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-24T10:52:37.21023Z","iopub.status.busy":"2023-11-24T10:52:37.209749Z","iopub.status.idle":"2023-11-24T10:53:57.848018Z","shell.execute_reply":"2023-11-24T10:53:57.846686Z","shell.execute_reply.started":"2023-11-24T10:52:37.21019Z"},"trusted":true},"outputs":[],"source":["explnr.lime(X=X_test,\n","            y=y_test,\n","            top_labels=1,\n","            num_samples=100,\n","            num_features=5,\n","            class_names=['Normal', 'Tuberculosis'],\n","            num_display=30,\n","            method='slic',\n","            random_mode=True,\n","            cmap='gray')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":891819,"sourceId":2332307,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
